<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Curious Joe</title>
    <link>/categories/r/</link>
    <description>Recent content in R on Curious Joe</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 29 Aug 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Have R Look After Your Stocks!</title>
      <link>/post/have-r-look-after-your-stock/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/have-r-look-after-your-stock/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#preparation&#34;&gt;Preparation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#libraries-used&#34;&gt;Libraries Used&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setting-up-environment-variables&#34;&gt;Setting up Environment Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fetching-stock-price&#34;&gt;Fetching Stock Price&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sending-sms&#34;&gt;Sending SMS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating-the-wrapper-function&#34;&gt;Creating the Wrapper Function&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#wrapper-function-for-sending-sms&#34;&gt;Wrapper Function for Sending SMS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrapper-function-for-stock-price-check&#34;&gt;Wrapper Function for Stock Price Check&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setting-up-an-r-job&#34;&gt;Setting up an R Job:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;“If you don’t find a way to make money while you sleep, you will work until you die.” - Warrent Buffet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you like to dabble in the stock market as a side hustle or just out of sheer curiosity, you may have faced the dilemma of how frequently you should check your stock price. The on and off checking of price can be a real productivity killer. If you are in that struggle bus then this article brings you good news! You can use R to do that boring job of constant checking and inform you when something interesting happens!&lt;/p&gt;
&lt;p&gt;In the coding world, there is a very popular principle called “Rule of Three” that states something along the line of&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you have to use something more than three times, you shouldn’t copy and paste the codes rather you put them in a procedure (function).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Checking stock price is also a series of tasks that you repeat every time you go and check the price. Which very easily can be automated using R!&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;The problem scenario we are trying to solve has two broad pieces to it:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Checking price movement of a set of stocks constantly&lt;/li&gt;
&lt;li&gt;Do something (by/sell) once something interesting (profit/loss) happens&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this tutorial we will get these two pieces done (actually partially doing the second step, since we won’t do any automated stock trading) by using the following tools:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Continuous price check: Using getQuote() from Quantmod package,&lt;/li&gt;
&lt;li&gt;Triggering a notification to do something: Using tw_send_message() from Twilio package.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;preparation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparation&lt;/h2&gt;
&lt;p&gt;Since we’ll use &lt;a href=&#34;https://www.twilio.com/&#34;&gt;Twilio’s&lt;/a&gt; free SMS service, we’ll first need to open a free account from Twilio’s website. It’s a very simple procedure. You can follow the instruction from this &lt;a href=&#34;https://www.youtube.com/watch?v=kTdMEc4LkKk#action=share&#34;&gt;video&lt;/a&gt; to set set up your Twilio account and get ready to use it with R. Make sure to get your personal phone number verified by Twilio to which number you want to have your stock movement notification sent.&lt;/p&gt;
&lt;p&gt;Once you have your account set up, store these two pieces of information from your Twilio account: SID and Token. R will need to use these two pieces of information to connect to your Twilio account.&lt;/p&gt;
&lt;div id=&#34;libraries-used&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Libraries Used&lt;/h3&gt;
&lt;p&gt;For this project we’ll use three libraries:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Twilio: to send message&lt;/li&gt;
&lt;li&gt;Quantmod: to get the stock prices&lt;/li&gt;
&lt;li&gt;dplyr: for data processing prcedures&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-up-environment-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting up Environment Variables&lt;/h3&gt;
&lt;p&gt;To use Twilio we need to set up two system environment variables in R: TWILIO_SID and TWILIO_TOKEN.&lt;/p&gt;
&lt;p&gt;We’ll use Sys.setenv() function to set these environment variables as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sys.setenv(TWILIO_SID = “TWILIO_SID_NUMBER”)&lt;br /&gt;
Sys.setenv(TWILIO_TOKEN = “TWILIO_TOKEN_NUMBER”)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# LIBRARIES ----
library(quantmod)
library(twilio)
library(dplyr)

Sys.setenv(TWILIO_SID = &amp;#39;ACf43caa6608884e01b935971dffce9d54&amp;#39;)
Sys.setenv(TWILIO_TOKEN = &amp;#39;2c68a2e9f06ac668e105f4c806374929&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fetching-stock-price&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fetching Stock Price&lt;/h2&gt;
&lt;p&gt;The main function that we’ll use for our purpose is the &lt;em&gt;getQuote()&lt;/em&gt; function from the Quantmod package. For example, to get the latest pricing detail about BestBuy’s stock you can call the function like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;quantmod::getQuote(‘BBY’)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantmod::getQuote(&amp;#39;BBY&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Trade Time   Last     Change    % Change   Open  High    Low
## BBY 2020-08-28 16:00:01 111.23 0.01000214 0.008993109 111.68 111.7 110.11
##      Volume
## BBY 2324806&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output will show you the time of the call, last price, day’s opening price, highest and lowest price for the day along with other information.&lt;/p&gt;
&lt;p&gt;For our purpose we’ll need to make a repeated call to this function thus we’ll put that &lt;em&gt;getQuote()&lt;/em&gt; inside a wrapper function which will repeatedly call this function to get the latest price of our selected stock(s).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sending-sms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sending SMS&lt;/h2&gt;
&lt;p&gt;We’ll use &lt;em&gt;tw_send_message()&lt;/em&gt; function from the Twilio package to send our notification message. To check if your Twilio setup is working properly, after loading the libraries and setting up system environment variables, you can call -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;tw_send_message(
to = “YOUR_VARIFIED_CELL_NO”,
from = “YOUR_TWILIO_CELL_NO”,
body = body
)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Make sure your use the cell no starting with country code and with no space or special character (e.g. +100756245) in between.&lt;/p&gt;
&lt;p&gt;For our purpose, we’ll need to modify the message body dynamically for every call thus we’ll put this &lt;em&gt;tw_send_message()&lt;/em&gt; function inside a customized function too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-wrapper-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating the Wrapper Function&lt;/h2&gt;
&lt;div id=&#34;wrapper-function-for-sending-sms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wrapper Function for Sending SMS&lt;/h3&gt;
&lt;p&gt;The following wrapper function &lt;em&gt;send_msg()&lt;/em&gt; will allow us to update the body of the message dynamically to show the stock name, updated price and other information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# SEND MESSAGE ----
send_msg = function(body){
  tw_send_message(
  to = Sys.getenv(&amp;quot;YOUR_VARIFIED_CELL_NO&amp;quot;),
  from = Sys.getenv(&amp;quot;YOUR_TWILIO_CELL_NO&amp;quot;),
  body = body
)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapper-function-for-stock-price-check&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wrapper Function for Stock Price Check&lt;/h3&gt;
&lt;p&gt;This wrapper function &lt;em&gt;stock_price_check()&lt;/em&gt; will allow us to automatically run &lt;em&gt;getQuote()&lt;/em&gt; function to get stock price detail about our desired stocks. &lt;em&gt;stock_price_check()&lt;/em&gt; is basically a while loop which will keep executing on certain intervals until the loop is stopped manually.&lt;/p&gt;
&lt;p&gt;The tasks that will be completed inside &lt;em&gt;stock_price_check()&lt;/em&gt; look like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;stock_price_check = function(tickers, price, quantity, threshold){&lt;br /&gt;
while(logic){&lt;br /&gt;
1. Fetch Stock Price&lt;br /&gt;
2. Calculate Profit
3. Print the Calculations in the R Console&lt;br /&gt;
4. Check if the Profit Meets the Set Threshold&lt;br /&gt;
4.1 If meets threshold: send a SMS&lt;br /&gt;
4.2 If doesn’t meet threshold: do nothing&lt;br /&gt;
5. Wait for a Certain Interval&lt;br /&gt;
6. Repeat Steps 1 to 5
}
}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fetch stock price and send SMS
# @param tickers A String vector of stock tickers 
# @param price A numeric vector of stock prices. In the same order of tickers
# @param quantity A numeric vector of stock quantity purchased. In the same order of tickers
# @param threshold A numeric input that shows the desired profit margin. Defaults to 4%


stock_price_check = function(tickers, price, quantity, threshold = 4){
  
  # setting up a counter. 
  i = 0 

  
  while(i &amp;lt; 2){ 
    
    # Step 1. Fetching stock price
    latest_price = quantmod::getQuote(tickers)
    
    # Step 2. Calculating profit
    latest_price[[&amp;#39;Buy&amp;#39;]] = c(price)
    latest_price[[&amp;quot;Q&amp;quot;]] = c(quantity)
    latest_price[[&amp;quot;Profit/Loss&amp;quot;]] = c(latest_price[[&amp;quot;Last&amp;quot;]] - latest_price[[&amp;quot;Buy&amp;quot;]])
    latest_price[[&amp;quot;Total_PL&amp;quot;]] = c(latest_price[[&amp;quot;Profit/Loss&amp;quot;]] * latest_price[[&amp;quot;Q&amp;quot;]])
    latest_price[[&amp;quot;Total_PL%&amp;quot;]] = c(round(latest_price[[&amp;quot;Total_PL&amp;quot;]] / (latest_price[[&amp;quot;Buy&amp;quot;]] * latest_price[[&amp;quot;Q&amp;quot;]]) * 100, 2))
    
    # Stp 3. Printing out calculations in R console
    print(latest_price[c(&amp;#39;Trade Time&amp;#39;, &amp;#39;Last&amp;#39;, &amp;#39;Buy&amp;#39;, &amp;#39;Total_PL&amp;#39;, &amp;#39;Total_PL%&amp;#39;)])
    cat(&amp;quot;\n&amp;quot;)
    
    # Step 4. Checking if the profit meets desired profit threshold
    df = as.data.frame(latest_price) %&amp;gt;%
      filter(`Total_PL%` &amp;gt; threshold) 
    
    if(nrow(df) &amp;gt; 0){
      # Step 4.1 Sending SMS if profit meets threshold
      msg = paste0(threshold, &amp;quot;% profit alert!\n&amp;quot;,  
                   paste0(rownames(df), 
                          &amp;quot;: Profit %: &amp;quot;, df[,&amp;quot;Total_PL%&amp;quot;], 
                          &amp;quot;| Total Profit: &amp;quot;, round(df[, &amp;quot;Total_PL&amp;quot;]),
                          collapse = &amp;#39; ; \n&amp;#39;))
      send_msg(body = msg)
      message(msg)
    }
  
  # Step 5. Waiting for certain interval (60 seconds)
  Sys.sleep(60)

  # Step Drop: Incrementing counter
  i = i + 1 # comment out this line when you run. I had to put an increment to make sure the while loop stops after 4 execusions
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s assume that we own a mini portfolio of two stocks &lt;strong&gt;Best Buy&lt;/strong&gt; (BBY - 14 stocks purchased at $114.09) and &lt;strong&gt;HP Enterprise&lt;/strong&gt; (HPE - 645 stocks purchased at $9.30). And we are interested to set up a reminder that will notify us once any of the stocks in the portfolio reaches to 4% profit margin.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that I have set the function to run only twice. Make sure the comment out the counter increment inside the function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stock_price_check(tickers = c(&amp;quot;BBY&amp;quot;, &amp;quot;HPE&amp;quot;), price = c(114.09, 9.30), quantity = c(14, 645))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Trade Time   Last    Buy Total_PL Total_PL%
## BBY 2020-08-28 16:00:01 111.23 114.09   -40.04     -2.51
## HPE 2020-08-28 16:03:43   9.83   9.30   341.85      5.70&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 4% profit alert!
## HPE: Profit %: 5.7| Total Profit: 342&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Trade Time   Last    Buy Total_PL Total_PL%
## BBY 2020-08-28 16:00:01 111.23 114.09   -40.04     -2.51
## HPE 2020-08-28 16:03:43   9.83   9.30   341.85      5.70&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 4% profit alert!
## HPE: Profit %: 5.7| Total Profit: 342&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-up-an-r-job&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting up an R Job:&lt;/h2&gt;
&lt;p&gt;Once this function is run &lt;em&gt;stock_price_check()&lt;/em&gt; it’ll keep running unless stopped. But it’ll occupy the R console which may not be ideal for everyone. In case you want to keep it running in the background you can use R Studio’s ‘Jobs’ functionality. Follow the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the “Tools” section and select “Start a local job”.&lt;/li&gt;
&lt;li&gt;For R script, browse to this script’s location and select this script&lt;/li&gt;
&lt;li&gt;Hit Start!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You should see R Studio will start the job which is basically calling this script and running it. Once it runs you should see the following two outputs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your portfolio details (Trade time, latest price, your purchase price, total profit $ and profit%) are being printed every minute,&lt;/li&gt;
&lt;li&gt;The SMS text that was sent to your mobile phone if a stock crossed the threshold profit margin.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;This is a very simple script that addresses problems for a very small stock portfolio. Some obvious and some not so obvious limitations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This stock price call doens’t work after hours. Once the official operation is close you’ll keep getting the last price during active hours repeatedly,&lt;/li&gt;
&lt;li&gt;The stock_price_check() function can be made more user friendly by adding the capability of getting portfolio details from a flat file. Which might be make it quite easy for people with larget stock portfolio.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Automatically Build Data Tables from US Census Survey!</title>
      <link>/post/automatically-build-data-tables-from-us-census-survey/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/automatically-build-data-tables-from-us-census-survey/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#brief-background-on-us-census-surveys&#34;&gt;Brief Background on US Census Surveys&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#preparation&#34;&gt;Preparation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#getting-your-api-key&#34;&gt;Getting your API Key&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setting-up-the-key&#34;&gt;Setting up the Key&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#calling-census-api-to-get-tract-household-income&#34;&gt;Calling Census API to Get Tract Household Income&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations-of-this-code&#34;&gt;Limitations of this Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overcoming-the-limitations&#34;&gt;Overcoming the Limitations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#making-the-state-name-input-flexible&#34;&gt;Making the state name input flexible&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-fall-back-capability-in-the-year-input&#34;&gt;Adding fall back capability in the year input&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-a-column-for-data-and-time&#34;&gt;Adding a column for data and time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#whats-next&#34;&gt;What’s next?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;US Census Bureau is probably the most reliable souce of demographic data on the US population. There are hundreds of use cases of the data from Us Census Bureau. In one of our recent projects we used household income data from Census as a proxy to missing actual household income data in our table.&lt;/p&gt;
&lt;p&gt;In this article, I’ll show how we got the income data from ACS5 survey using the ‘tidycensus’ package. In doing so, I’ll show how to write your own function to wrap the out of the box function to serve your customised purpose. As bonus I’ll briefly describe how you may automate this entire process so that you can run this process from a server, save the data in your database and each year automatically update the table with the new Census data.&lt;/p&gt;
&lt;div id=&#34;brief-background-on-us-census-surveys&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Brief Background on US Census Surveys&lt;/h2&gt;
&lt;p&gt;There are different types of surveys conducted by the Census Bureau. Not all of them have the same data or data at the same level based on their nature. Here you can find details about the different survey types and other details: &lt;a href=&#34;https://www.census.gov/programs-surveys.html&#34; class=&#34;uri&#34;&gt;https://www.census.gov/programs-surveys.html&lt;/a&gt;. For our purpose we needed a survey that has household income data at the most grannular geographic level. We ended up using the American Community Survey &lt;a href=&#34;https://www.census.gov/data/developers/data-sets/acs-5year.html&#34;&gt;ACS5 survey&lt;/a&gt; because this survey contains household income data at the Census &lt;a href=&#34;https://www.census.gov/content/dam/Census/data/developers/geoareaconcepts.pdf&#34;&gt;Tract&lt;/a&gt; level which is the most grannular level for which household income data is available in census.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preparation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparation&lt;/h2&gt;
&lt;div id=&#34;getting-your-api-key&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting your API Key&lt;/h3&gt;
&lt;p&gt;For this project we have used an R package called &lt;em&gt;tidyverse&lt;/em&gt;. Behind the scene &lt;em&gt;tidycensus&lt;/em&gt; package calls the API provided by the Census Bureau. To call this API you need to have a key, which is basically is unique ID that is automatically generated against each new user by the Census Bureau. You can get a key from here: &lt;a href=&#34;https://api.census.gov/data/key_signup.html&#34; class=&#34;uri&#34;&gt;https://api.census.gov/data/key_signup.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once you have the key save it in a separte text file or an r script. So that later on we can load the file and use that key in our functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-up-the-key&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting up the Key&lt;/h3&gt;
&lt;p&gt;Once you have the API key available you need to set it up to be used by the functions from tidycensus package. You can set it up using following line of code:&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;census_api_key(&amp;#39;YOUR_API_KEY&amp;#39;, install = FALSE, overwrite = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have all the prerequisits set to call the Census API.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;calling-census-api-to-get-tract-household-income&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calling Census API to Get Tract Household Income&lt;/h2&gt;
&lt;p&gt;Using the following line of code you can get all the tract level household income data for an entire state in the US. We are using this variable &lt;em&gt;B19013_001&lt;/em&gt; from the ACS5 survey. You can learn more about all the available variables going to this link: &lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/variables.html&#34; class=&#34;uri&#34;&gt;https://api.census.gov/data/2018/acs/acs5/variables.html&lt;/a&gt;
So to get tract level median household income in IL for the ACS5 survey of 2018 you can run the following line of code:&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;get_acs(state = &amp;#39;IL&amp;#39;, year = 2018, geography = &amp;#39;tract&amp;#39;, variables = &amp;#39;B19013_001&amp;#39;, 
                                     geometry = FALSE, survey = &amp;#39;acs5&amp;#39;, show_call = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Explanation of the code&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;get_acs()&lt;/em&gt; function is used to call the API for the ACS surveys. You should read through the code description for details but here is the brief description of the parameters used in this case:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;state: abbreviated name or names of the state(s),&lt;/li&gt;
&lt;li&gt;year: for which year’s ACS survey you are looking for.&lt;/li&gt;
&lt;li&gt;geography: at which geographic level you are looking at. We are looking for &lt;em&gt;tract&lt;/em&gt; level data. Other options are county, block etc.&lt;/li&gt;
&lt;li&gt;survey: which survey you are looking for. We are looking for **ACS5* survey,&lt;/li&gt;
&lt;li&gt;show_call: setting it up as true prints the message output in the R console while the API is called. Really helpful when you put this function in production to check back reason behind in case the function fails.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations-of-this-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limitations of this Code&lt;/h2&gt;
&lt;p&gt;Our goal is to fetch that data, store the data in a database and then schedule that script to be re-run automatically every year. Using this out of the box function doesn’t serve that entire purpose because of these limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It has to have a state name as an input. We could make it dynamic so that we could use it to load all states’ data or just a selective set of states or just one state,&lt;/li&gt;
&lt;li&gt;We want to schedule this to run automatically at a specific date. But since the ACS5 survey publication date is not exactly same each year, we need to have some flexibility so that in case our desired year’s survey isn’t populated the function falls back to fetch latest available data.&lt;/li&gt;
&lt;li&gt;We would like to have a column with the date and time recorded when this data is called and store that along with other data in the database table.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the next sections I’ll walk you through writing a customised functions which will address these limitations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;overcoming-the-limitations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overcoming the Limitations&lt;/h2&gt;
&lt;p&gt;Before we start to building the elaborate function, I’ll start with a basic wrapper function. And then we’ll keep adding additional argumnts to it to overcome the limitations.&lt;/p&gt;
&lt;p&gt;Here’s how R function skeleton looks like:&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt; functionName = function(input01, input2){
                  Logic
        }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You give it a name so that in future you can save the function and reuse it. Inside the function() within parentheses you include the input variable name(s). And you write the logic inside the curly braces.&lt;/p&gt;
&lt;p&gt;Now let’s write a basic function to wrap the two pieces of codes we have written earlier to get ACS data:&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt; getAcsIncome = function(names, year, KEY = &amp;#39;YOUR_KEY&amp;#39;){
        ## setting up API call key
        census_api_key(apiKey, install = FALSE, overwrite = TRUE)
        
        ## calling get_acs()
        get_acs(state = names, year = year, geography = &amp;#39;tract&amp;#39;, variables = &amp;#39;B19013_001&amp;#39;, 
                                     geometry = FALSE, survey = &amp;#39;acs5&amp;#39;, show_call = TRUE)
        }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have saved my API KEY in a separate script. So I have loaded the script and using the KEY from the script to get track level data for IL from 2018 ACS5 survey.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Loading libraries and key
library(tidycensus)
source(&amp;#39;KEY.R&amp;#39;)

# Wrapper function
getAcsIncome = function(names, year, KEY){
        ## setting up API call key
        census_api_key(key = API_KEY, install = FALSE, overwrite = TRUE)
        
        ## calling get_acs()
        get_acs(state = names, year = year, geography = &amp;#39;tract&amp;#39;, variables = &amp;#39;B19013_001&amp;#39;, 
                                     geometry = FALSE, survey = &amp;#39;acs5&amp;#39;, show_call = TRUE)
}

# Calling the function and display glimpse of result 
IL_HH_Income = getAcsIncome(names = &amp;#39;IL&amp;#39;, year = 2018, KEY = API_KEY)
head(IL_HH_Income)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   GEOID       NAME                                      variable  estimate   moe
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                                     &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 17001000100 Census Tract 1, Adams County, Illinois    B19013_0~    44613  6384
## 2 17001000201 Census Tract 2.01, Adams County, Illinois B19013_0~    44878  4356
## 3 17001000202 Census Tract 2.02, Adams County, Illinois B19013_0~    46964 10202
## 4 17001000400 Census Tract 4, Adams County, Illinois    B19013_0~    33750  7386
## 5 17001000500 Census Tract 5, Adams County, Illinois    B19013_0~    38526  4846
## 6 17001000600 Census Tract 6, Adams County, Illinois    B19013_0~    51491 10117&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;making-the-state-name-input-flexible&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Making the state name input flexible&lt;/h3&gt;
&lt;p&gt;Now we have a operating function, we’ll move to the next steps where we’ll add first set of arguments to it to make the state name input flexible.&lt;/p&gt;
&lt;p&gt;We’ll use a built-in constant in R namely: state.abb. It includes the 50 state name abbreviations. In our customised wrapper function we’ll add changes to address these following use cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;download all states data when input is ‘all’/‘ALL’&lt;/li&gt;
&lt;li&gt;download selected state(s) data when input is one/multiple state names in abbreviations&lt;/li&gt;
&lt;li&gt;provide an error message if provided input doesn’t match any of the above two input types&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Wrapper function
getAcsIncome = function(names, year, KEY){
        ## setting up API call key
        census_api_key(key = API_KEY, install = FALSE, overwrite = TRUE)
        
        ## setting up blank array to store state names 
        stateNames = NULL
        
        # when all states are required
        if(names %in% c(&amp;#39;all&amp;#39;, &amp;#39;ALL&amp;#39;)){
          stateNames = state.abb
        } 
        
        # when specific state or states are mentioned in names
        else if(names %in% c(state.abb)){
          stateNames = names
        }
        
        # in any other cases
        else{
          print(&amp;quot;Provide a value in stateNames variable. Available options: all/ALL/any of the 50 states (abb.)&amp;quot;)
        }
  
        ## calling get_acs()
        get_acs(state = stateNames, year = year, geography = &amp;#39;tract&amp;#39;, variables = &amp;#39;B19013_001&amp;#39;, geometry = FALSE, survey = &amp;#39;acs5&amp;#39;, show_call = TRUE)
}

head(getAcsIncome(names = &amp;#39;all&amp;#39;, year = 2018, KEY = API_KEY))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   GEOID       NAME                                      variable  estimate   moe
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                                     &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 01001020100 Census Tract 201, Autauga County, Alabama B19013_0~    58625 14777
## 2 01001020200 Census Tract 202, Autauga County, Alabama B19013_0~    43531  6053
## 3 01001020300 Census Tract 203, Autauga County, Alabama B19013_0~    51875  8744
## 4 01001020400 Census Tract 204, Autauga County, Alabama B19013_0~    54050  5166
## 5 01001020500 Census Tract 205, Autauga County, Alabama B19013_0~    72417 14919
## 6 01001020600 Census Tract 206, Autauga County, Alabama B19013_0~    46688 13043&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-fall-back-capability-in-the-year-input&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding fall back capability in the year input&lt;/h3&gt;
&lt;p&gt;To add that capability we’ll use a package called &lt;em&gt;tryCatchLog&lt;/em&gt;. The basic sceleton of tryCatch() function that we’ll use is like following:&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;
result = tryCatch({
    expr
}, warning = function(w) {
    warning-handler-code
}, error = function(e) {
    error-handler-code
}, finally = {
    cleanup-code
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here inside the curly braces you add the code to evaluate and inside second function, following warning/error, provide the logic to execute if the first code block fails. The above skeleton was copied from this &lt;a href=&#34;http://mazamascience.com/WorkingWithData/?p=912&#34;&gt;article&lt;/a&gt;. That article has a more detailed discussion on how to apply try catch function.&lt;/p&gt;
&lt;p&gt;In our case we’ll use trycatch function to update a variable. Then we’ll add additional code block that will run based on the value of that variable. Also if the first code block fails, we’ll print out a message where the error message will be printed starting with the date showing which year it tried.&lt;/p&gt;
&lt;p&gt;The tryCatch block of our code inside the function will look like following:&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;
  # starting with variable: an.error.occured with value of FALSE
  an.error.occured &amp;lt;- FALSE
  tryCatch({
    
    # trying for current year - 2 
    year = as.numeric(substr(Sys.Date(), start = 1, stop = 4)) - 2
    
    # calling api to get data
    data = tidycensus::get_acs(state = name, year = year, geography = &amp;#39;tract&amp;#39;, variables = &amp;#39;B19013_001&amp;#39;, geometry = FALSE, survey = &amp;#39;acs5&amp;#39;, show_call = TRUE)
    }, error = function(e) {
    
    # updating the variable
    an.error.occured &amp;lt;&amp;lt;- TRUE
    # printing out error message to be stored in log with the 
    message(paste0(&amp;quot;Year tried: &amp;quot;, year, &amp;quot;/n&amp;quot;, e))})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above block we are capturing if our first try of the code block fails. If it fails we are updating &lt;em&gt;an.error.occured&lt;/em&gt; variable to TRUE. Which will trigger the next block where we’ll use one year older year value.&lt;/p&gt;
&lt;p&gt;Eventually the final function with the added full trycatch functionality will look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getAcsIncome = function(names, year, KEY){
  
   ## setting up API call key
        census_api_key(key = API_KEY, install = FALSE, overwrite = TRUE)
        
        ## setting up blank array to store state names 
        stateNames = NULL
        
        # when all states are required
        if(names %in% c(&amp;#39;all&amp;#39;, &amp;#39;ALL&amp;#39;)){
          stateNames = state.abb
        } 
        
        # when specific state or states are mentioned in names
        else if(names %in% c(state.abb)){
          stateNames = names
        }
        
        # in any other cases
        else{
          print(&amp;quot;Provide a value in stateNames variable. Available options: all/ALL/any of the 50 states (abb.)&amp;quot;)
        }
  
  # starting with variable: an.error.occured with value of FALSE
  an.error.occured &amp;lt;- FALSE
  tryCatch({
    
    # calling api to get data
    data = tidycensus::get_acs(state = stateNames, year = year, geography = &amp;#39;tract&amp;#39;, variables = &amp;#39;B19013_001&amp;#39;, geometry = FALSE, survey = &amp;#39;acs5&amp;#39;, show_call = TRUE)
    }, error = function(e) {
    
    # updating the variable
    an.error.occured &amp;lt;&amp;lt;- TRUE
    # printing out error message to be stored in log
    message(paste0(&amp;quot;Year tried: &amp;quot;, year, &amp;quot;\n&amp;quot;, e))})
  
  
  #  try for 2 year older data
  if(an.error.occured == TRUE){
    year = year - 2
    
    # calling api to get data
    data = tidycensus::get_acs(state = stateNames, year = year, geography = &amp;#39;tract&amp;#39;, variables = &amp;#39;B19013_001&amp;#39;, geometry = FALSE, survey = &amp;#39;acs5&amp;#39;, show_call = TRUE)
  }
  
  ## returning resulting data
  return(data)
}

head(getAcsIncome(names = &amp;#39;IL&amp;#39;, year = 2020, KEY = API_KEY))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## To install your API key for use in future sessions, run this function with `install = TRUE`.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Getting data from the 2016-2020 5-year ACS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Census API call: https://api.census.gov/data/2020/acs/acs5?get=B19013_001E%2CB19013_001M%2CNAME&amp;amp;for=tract%3A%2A&amp;amp;in=state%3A17&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Year tried: 2020
## Error: Your API call has errors.  The API message returned is &amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;Error report&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;HTTP Status 404 - /data/2020/acs/acs5&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Getting data from the 2014-2018 5-year ACS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Census API call: https://api.census.gov/data/2018/acs/acs5?get=B19013_001E%2CB19013_001M%2CNAME&amp;amp;for=tract%3A%2A&amp;amp;in=state%3A17&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   GEOID       NAME                                      variable  estimate   moe
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                                     &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 17001000100 Census Tract 1, Adams County, Illinois    B19013_0~    44613  6384
## 2 17001000201 Census Tract 2.01, Adams County, Illinois B19013_0~    44878  4356
## 3 17001000202 Census Tract 2.02, Adams County, Illinois B19013_0~    46964 10202
## 4 17001000400 Census Tract 4, Adams County, Illinois    B19013_0~    33750  7386
## 5 17001000500 Census Tract 5, Adams County, Illinois    B19013_0~    38526  4846
## 6 17001000600 Census Tract 6, Adams County, Illinois    B19013_0~    51491 10117&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Among the messages printed, this following message block shows that our code block inside the trycatch function failed. Then it fell back to 2 year’s older data. The reason is the latest survey data available in ACS5 is for 2018.&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;## Year tried: 2020
## Error: Your API call has errors.  The API message returned is &amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;Error report&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;HTTP Status 404 - /data/2020/acs/acs5&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we move on to adding our next argument block to overcome the final limitation, we need to make one more change. Since our eventual goal is to run this function from a server, let’s make the year input embedded inside the function.&lt;/p&gt;
&lt;p&gt;We’ll introduce a variable named &lt;em&gt;year&lt;/em&gt; inside the function with a default value of (current year - 2) value and then in the fall back we’ll update that variable to (current year - 3). Which will make sure that whenver we run the code, it’ll ask for the 2 year older data and even if that 2 year data is not available it’ll call for 3 year older data.&lt;/p&gt;
&lt;p&gt;Here’s the two lines of codes that will be added:&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;# creating year variable with default value
    year = as.numeric(substr(Sys.Date(), start = 1, stop = 4)) - 2
    
    #updating year variable
    year = as.numeric(substr(Sys.Date(), start = 1, stop = 4)) - 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the final code chunk with that year functionality added.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-a-column-for-data-and-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding a column for data and time&lt;/h3&gt;
&lt;p&gt;This is the simplest part of this tutorial. Basically we’ll add Sys.time() as an additional column to the already fetched data.&lt;/p&gt;
&lt;p&gt;Here’s the final code chunk:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getAcsIncome = function(names, KEY){
  
   ## setting up API call key
        census_api_key(key = API_KEY, install = FALSE, overwrite = TRUE)
        
        ## setting up blank array to store state names 
        stateNames = NULL
        
        # when all states are required
        if(names %in% c(&amp;#39;all&amp;#39;, &amp;#39;ALL&amp;#39;)){
          stateNames = state.abb
        } 
        
        # when specific state or states are mentioned in names
        else if(names %in% c(state.abb)){
          stateNames = names
        }
        
        # in any other cases
        else{
          print(&amp;quot;Provide a value in stateNames variable. Available options: all/ALL/any of the 50 states (abb.)&amp;quot;)
        }
  
  # starting with variable: an.error.occured with value of FALSE
  an.error.occured &amp;lt;- FALSE
  tryCatch({
    
    # creating year variable with default value
    year = as.numeric(substr(Sys.Date(), start = 1, stop = 4)) - 2
  
    # calling api to get data
    data = tidycensus::get_acs(state = stateNames, year = year, geography = &amp;#39;tract&amp;#39;, variables = &amp;#39;B19013_001&amp;#39;, geometry = FALSE, survey = &amp;#39;acs5&amp;#39;, show_call = TRUE)
    }, error = function(e) {
    
    # updating the variable
    an.error.occured &amp;lt;&amp;lt;- TRUE
    # printing out error message to be stored in log
    message(paste0(&amp;quot;Year tried: &amp;quot;, year, &amp;quot;\n&amp;quot;, e))})
  
  
  #  try for 2 year older data
  if(an.error.occured == TRUE){
    
    #updating year variable
    year = as.numeric(substr(Sys.Date(), start = 1, stop = 4)) - 3
    
    # calling api to get data
    data = tidycensus::get_acs(state = stateNames, year = year, geography = &amp;#39;tract&amp;#39;, variables = &amp;#39;B19013_001&amp;#39;, geometry = FALSE, survey = &amp;#39;acs5&amp;#39;, show_call = TRUE)
  }
  
  # adding update data to a column
  data$UPDATE_DATE = Sys.time()
  
  ## returning resulting data
  return(data)
}

summary(getAcsIncome(names = &amp;#39;all&amp;#39;, KEY = API_KEY))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     GEOID               NAME             variable            estimate     
##  Length:72877       Length:72877       Length:72877       Min.   :  2499  
##  Class :character   Class :character   Class :character   1st Qu.: 42353  
##  Mode  :character   Mode  :character   Mode  :character   Median : 57099  
##                                                           Mean   : 64289  
##                                                           3rd Qu.: 78323  
##                                                           Max.   :250001  
##                                                           NA&amp;#39;s   :1013    
##       moe          UPDATE_DATE                 
##  Min.   :   550   Min.   :2020-07-10 09:03:57  
##  1st Qu.:  6051   1st Qu.:2020-07-10 09:03:57  
##  Median :  8711   Median :2020-07-10 09:03:57  
##  Mean   : 10212   Mean   :2020-07-10 09:03:57  
##  3rd Qu.: 12521   3rd Qu.:2020-07-10 09:03:57  
##  Max.   :126054   Max.   :2020-07-10 09:03:57  
##  NA&amp;#39;s   :1092&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-next&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;There are two things left now to set this script in a server to be run automatically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adding log file. Anytime you want to keep a script running from a server, you should consider adding logging capability to it. It’ll come real handy to debug in case the script fails.&lt;/li&gt;
&lt;li&gt;Automating this script. One easy way in Windows is to use windows’ task scheduler. You can take a look at my other tutorial[Automate Your Repetitive Reports!]](&lt;a href=&#34;https://curious-joe.net/post/automate-your-repetitive-reports/&#34; class=&#34;uri&#34;&gt;https://curious-joe.net/post/automate-your-repetitive-reports/&lt;/a&gt;) to know detail about how to automate a script using windows task scheduler.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;US Census Bureau is a great source of data on the US population. There are all sorts of interesting data available such as unemployment data, race related data, education related data and so on. All you need to do is to go through the documentation for the variable which I linked earler, here’s &lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/variables.html&#34;&gt;again&lt;/a&gt;. Hope this tutorial makes your census bureau data exploration journey easier and more useful in case you want to use that data continuously.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is that Red Wine Good Enough?</title>
      <link>/post/is-the-red-wine-good-enough/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/is-the-red-wine-good-enough/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-data&#34;&gt;Exploring Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-features&#34;&gt;Exploring Features&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#transforming-target-feature&#34;&gt;Transforming Target Feature&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-predictors-visually&#34;&gt;Exploring Predictors Visually&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#checking-correlation&#34;&gt;Checking Correlation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering&#34;&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fitting-model&#34;&gt;Fitting Model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#splitting-data&#34;&gt;Splitting Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fitting-model-on-training-data&#34;&gt;Fitting Model on Training Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#checking-model-performance&#34;&gt;Checking Model Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-inisght&#34;&gt;Summary Inisght&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Let’s assume that we have been hired by a winery to build a predictive model to check the qulity of their red wine. The traiditional way of wine testing is done by a human expert. Thus the process is prone to human error. The goal is to establish a process of producing an objective method of wine testing and combining that with the existing process to reduce human error.&lt;/p&gt;
&lt;p&gt;For the purpose of building the predictive model, we’ll use a dataset provided by UCI machine learning repository. We’ll try to predict wine quality based on features associated with wine.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore the data&lt;/li&gt;
&lt;li&gt;Predict the wine quality (binary classification)&lt;/li&gt;
&lt;li&gt;Explore model result&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;exploring-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exploring Data&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Loading data, libraries and primary glimpsing over data&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# libraries
library(dplyr)
library(ggplot2)
library(caTools)
library(caret)
library(GGally)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataFrame = read.csv(&amp;quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&amp;quot;, sep = &amp;#39;;&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(dataFrame)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  fixed.acidity   volatile.acidity  citric.acid    residual.sugar  
##  Min.   : 4.60   Min.   :0.1200   Min.   :0.000   Min.   : 0.900  
##  1st Qu.: 7.10   1st Qu.:0.3900   1st Qu.:0.090   1st Qu.: 1.900  
##  Median : 7.90   Median :0.5200   Median :0.260   Median : 2.200  
##  Mean   : 8.32   Mean   :0.5278   Mean   :0.271   Mean   : 2.539  
##  3rd Qu.: 9.20   3rd Qu.:0.6400   3rd Qu.:0.420   3rd Qu.: 2.600  
##  Max.   :15.90   Max.   :1.5800   Max.   :1.000   Max.   :15.500  
##    chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      
##  Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  
##  1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956  
##  Median :0.07900   Median :14.00       Median : 38.00       Median :0.9968  
##  Mean   :0.08747   Mean   :15.87       Mean   : 46.47       Mean   :0.9967  
##  3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9978  
##  Max.   :0.61100   Max.   :72.00       Max.   :289.00       Max.   :1.0037  
##        pH          sulphates         alcohol         quality     
##  Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000  
##  1st Qu.:3.210   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  
##  Median :3.310   Median :0.6200   Median :10.20   Median :6.000  
##  Mean   :3.311   Mean   :0.6581   Mean   :10.42   Mean   :5.636  
##  3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  
##  Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the features we see ‘quality’ is our target feature. And we have total 11 features to be used as the predictors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-features&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exploring Features&lt;/h1&gt;
&lt;div id=&#34;transforming-target-feature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transforming Target Feature&lt;/h2&gt;
&lt;p&gt;Since we will cover talk about the classification model, we’ll convert our target feature from continuous to binary class. So that we would be able to fit one of the very widely used yet very easy classification models.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Distribution of original target feature labels&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# checking ratio of different labels in target feature
prop.table(table(dataFrame$quality))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##           3           4           5           6           7           8 
## 0.006253909 0.033145716 0.425891182 0.398999375 0.124452783 0.011257036&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataFrame = dataFrame %&amp;gt;%
  mutate(quality_bin = as.factor(ifelse(quality &amp;lt;= 5, 0,1))) %&amp;gt;%
  select(-quality)


p = round(prop.table(table(dataFrame$quality_bin))*100,2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After tranformation we have 53.47% cases classified records as good wines vs 46.53% as bad wines.&lt;/p&gt;
&lt;p&gt;We have a nice distribution of our target classes here! Which is very nice. Otherwise, we would’ve had to deal with &lt;em&gt;Data Balancing&lt;/em&gt;. Though we won’t cover that area in this tutorial, it’s a great discussion area to delve into. So some extra points for those who’ll learn about it!&lt;/p&gt;
&lt;p&gt;In short, we would like to &lt;strong&gt;have a balanced distribution of observations from different labels in our target feature&lt;/strong&gt;. Otherwise, some ML algorithms tend to overfit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-predictors-visually&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring Predictors Visually&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Exploring acidity&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataFrame %&amp;gt;%
  ggplot(aes(x = as.factor(quality_bin), y = fixed.acidity, color = quality_bin)) +
  geom_boxplot(outlier.color = &amp;quot;darkred&amp;quot;, notch = FALSE) +
  ylab(&amp;quot;Acidity&amp;quot;) + xlab(&amp;quot;Quality (1 = good, 2 = bad)&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;, axis.title.x = element_blank()) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/viz_acidity-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have multiple features that are continuous and can plot them similarly. Which means we’ll have to re write the code that we have just wrote in code chunk: viz_acidity again and again. In coding, we don’t want to do that. So we’ll create a function and wrap that around our code so that it can be reused in future!&lt;/p&gt;
&lt;p&gt;If it sounds too much, just stick with it. Once you see the code, it’ll make a lot more sense.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# boxplot_viz
# plots continuous feature in boxplot categorized on the quality_bin feature labels from dataFrame 
# @param feat Feature name (string) to be plotted
boxplot_viz = function(feat){

  dataFrame %&amp;gt;%
    ggplot(aes_string(x = as.factor(&amp;#39;quality_bin&amp;#39;), y = feat, color = &amp;#39;quality_bin&amp;#39;)) +
    geom_boxplot(outlier.color = &amp;quot;darkred&amp;quot;, notch = FALSE) +
    labs(title = paste0(&amp;quot;Boxplot of feature: &amp;quot;, feat)) + ylab(feat) + xlab(&amp;quot;Quality (1 = good, 2 = bad)&amp;quot;) + 
    theme(legend.position = &amp;quot;none&amp;quot;, axis.title.x = element_blank()) + 
    theme_minimal()
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot_viz(&amp;#39;volatile.acidity&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in names(dataFrame %&amp;gt;% select(-&amp;#39;quality_bin&amp;#39;))){
  print(boxplot_viz(i))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-5.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-6.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-7.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-8.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-9.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-10.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Checking Correlation&lt;/h2&gt;
&lt;p&gt;We can quickly check correlations among our predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataFrame %&amp;gt;% 
  # correlation plot 
  ggcorr(method = c(&amp;#39;complete.obs&amp;#39;,&amp;#39;pearson&amp;#39;), 
         nbreaks = 6, digits = 3, palette = &amp;quot;RdGy&amp;quot;, label = TRUE, label_size = 3, 
         label_color = &amp;quot;white&amp;quot;, label_round = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/correlation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Highly correlated features don’t add new information to the model and blurrs the effect of individual feature on the predictor and thus makes it difficult to explain effect of individual features on target feature. This problem is called &lt;strong&gt;Multicollinearity&lt;/strong&gt;. As a general rule, we don’t want to keep features with very high correlation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What should be the threshold of correlation?&lt;/li&gt;
&lt;li&gt;How do we decide which variable to drop?&lt;/li&gt;
&lt;li&gt;Do correlated features hurt predictive accuracy?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these are great questions and worth having a good understanding about. So again extra points for those who’ll learn about !&lt;/p&gt;
&lt;p&gt;Before making any decision based on correlation, check distribution of the feature. Unless any two features have a linear relation, correlation doesn’t mean much.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Feature Engineering&lt;/h1&gt;
&lt;p&gt;Based on the insight gained from the data exploration, some features may need to be transformed or new features can be created. Some common feature engineering tasks are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normalization and standardization of features&lt;/li&gt;
&lt;li&gt;Binning continuous features&lt;/li&gt;
&lt;li&gt;Creating composit features&lt;/li&gt;
&lt;li&gt;Creating dummy variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial won’t cover &lt;em&gt;feature engineering&lt;/em&gt; but it’s a great area to explore. A great data exploration followed by necessary feature engineering are the absolute necessary prerequisites before fitting any predictive model!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting Model&lt;/h1&gt;
&lt;div id=&#34;splitting-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splitting Data&lt;/h2&gt;
&lt;p&gt;In practical world we train our predictive models on historical data which is called &lt;strong&gt;Training Data&lt;/strong&gt;. Then we apply that model on new unseen data, called &lt;strong&gt;Test Data&lt;/strong&gt;, and measure the performance. thus we can be sure that our model is stable or not over fitted on training data. But since we won’t have access to new wine data, we’ll split our dataset into training and testing data on a 80:20 ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
split = sample.split(dataFrame$quality_bin, SplitRatio = 0.80)
training_set = subset(dataFrame, split == TRUE)
test_set = subset(dataFrame, split == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check the data balance in training and test data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop.table(table(training_set$quality_bin))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##         0         1 
## 0.4652072 0.5347928&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop.table(table(test_set$quality_bin))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##        0        1 
## 0.465625 0.534375&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-model-on-training-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting Model on Training Data&lt;/h2&gt;
&lt;p&gt;We’ll fit &lt;strong&gt;Logistic Regression&lt;/strong&gt; classification model on our dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_log = glm(quality_bin ~ ., 
                data = training_set, family = &amp;#39;binomial&amp;#39;)
summary(model_log)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = quality_bin ~ ., family = &amp;quot;binomial&amp;quot;, data = training_set)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.3688  -0.8309   0.2989   0.8109   2.4184  
## 
## Coefficients:
##                        Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)           17.369521  90.765368   0.191  0.84824    
## fixed.acidity          0.069510   0.112062   0.620  0.53507    
## volatile.acidity      -3.602258   0.558889  -6.445 1.15e-10 ***
## citric.acid           -1.543276   0.638161  -2.418  0.01559 *  
## residual.sugar         0.012106   0.060364   0.201  0.84106    
## chlorides             -4.291590   1.758614  -2.440  0.01467 *  
## free.sulfur.dioxide    0.027452   0.009293   2.954  0.00314 ** 
## total.sulfur.dioxide  -0.016723   0.003229  -5.180 2.22e-07 ***
## density              -23.425390  92.700349  -0.253  0.80050    
## pH                    -0.977906   0.828710  -1.180  0.23799    
## sulphates              3.070254   0.532655   5.764 8.21e-09 ***
## alcohol                0.946654   0.120027   7.887 3.10e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1766.9  on 1278  degrees of freedom
## Residual deviance: 1301.4  on 1267  degrees of freedom
## AIC: 1325.4
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s plot the variables with the lowest p values/highest absolute z value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p = varImp(model_log) %&amp;gt;% data.frame() 
p = p %&amp;gt;% mutate(Features = rownames(p)) %&amp;gt;% arrange(desc(Overall)) %&amp;gt;% mutate(Features = tolower(Features))

p %&amp;gt;% ggplot(aes(x = reorder(Features, Overall), y = Overall)) + geom_col(width = .50, fill = &amp;#39;darkred&amp;#39;) + coord_flip() + 
  labs(title = &amp;quot;Importance of Features&amp;quot;, subtitle = &amp;quot;Based on the value of individual z score&amp;quot;) +
  xlab(&amp;quot;Features&amp;quot;) + ylab(&amp;quot;Abs. Z Score&amp;quot;) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/featImp-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-model-performance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Checking Model Performance&lt;/h1&gt;
&lt;p&gt;We’ll check how our model performs by running it on our previously unseen test data. We’ll compare the predicted outcome with the actual outcome and calculate some typically used binary classification model performance measuring metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predict target feature in test data
y_pred = as.data.frame(predict(model_log, type = &amp;quot;response&amp;quot;, newdata = test_set)) %&amp;gt;% 
  structure( names = c(&amp;quot;pred_prob&amp;quot;)) %&amp;gt;%
  mutate(pred_cat = as.factor(ifelse(pred_prob &amp;gt; 0.5, &amp;quot;1&amp;quot;, &amp;quot;0&amp;quot;))) %&amp;gt;% 
  mutate(actual_cat = test_set$quality_bin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p = confusionMatrix(y_pred$pred_cat, y_pred$actual_cat, positive = &amp;quot;1&amp;quot;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 108  46
##          1  41 125
##                                           
##                Accuracy : 0.7281          
##                  95% CI : (0.6758, 0.7761)
##     No Information Rate : 0.5344          
##     P-Value [Acc &amp;gt; NIR] : 9.137e-13       
##                                           
##                   Kappa : 0.4548          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 0.668           
##                                           
##             Sensitivity : 0.7310          
##             Specificity : 0.7248          
##          Pos Pred Value : 0.7530          
##          Neg Pred Value : 0.7013          
##              Prevalence : 0.5344          
##          Detection Rate : 0.3906          
##    Detection Prevalence : 0.5188          
##       Balanced Accuracy : 0.7279          
##                                           
##        &amp;#39;Positive&amp;#39; Class : 1               
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;font color = maroon&gt;&lt;strong&gt;Model Perfomance Summary:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;: 72.81% of the wine samples have been classified correctly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensitivity/Recall&lt;/strong&gt;: 73.1% of the actual good wine samples have been classified correctly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pos Pred Value/Precision&lt;/strong&gt;: 75.3% of the total good wine predictions are actually good wines. &lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-inisght&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary Inisght&lt;/h1&gt;
&lt;p&gt;So let’s summarize about what we have learned about wine testing from our exercise:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alchohol content, Volatile Acidity, Sulphate and total Sulpher Dioxide are the top four most statistically significant features that affect wine quality.&lt;/li&gt;
&lt;li&gt;Given the information about the 11 features that we have analyzed, we can accurately predict wine quality in about 73% of the cases,&lt;/li&gt;
&lt;li&gt;Which is about 26% more accurate than the accuracy achieved by using traditional expert based method.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;a href=&#34;https://www.theguardian.com/lifeandstyle/2013/jun/23/wine-tasting-junk-science-analysis&#34;&gt;“People could tell the difference between wines under £5 and those above £10 only 53% of the time for whites and only 47% of the time for reds.”&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;center&gt;
&lt;div&gt;
&lt;p&gt;&lt;img src=&#34;https://media1.tenor.com/images/f050405657f9ac48d3b976051436e885/tenor.gif?itemid=10577278&#34; width=&#34;400&#34;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;div class=&#34;tocify-extend-page&#34; data-unique=&#34;tocify-extend-page&#34; style=&#34;height: 0;&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Automate Your Repetitive Reports!</title>
      <link>/post/automate-your-repetitive-reports/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/automate-your-repetitive-reports/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-r-script-automation-and-why&#34;&gt;What is R script automation and why?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#outline-of-what-well-do-here&#34;&gt;Outline of what we’ll do here&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#part01-creating-the-report&#34;&gt;Part01: Creating the report:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part02-automating-the-report-reproduction&#34;&gt;Part02: Automating the report reproduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;what-is-r-script-automation-and-why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is R script automation and why?&lt;/h2&gt;
&lt;p&gt;In most use cases R is used to analyze data, run statistical tests and build model. In doing so, data scientists constantly interact with R by writing codes and produce results from these interactions. Eventually, these results are stored, shared or presented as a report. But what if you have to reproduce the report every day or in other type of regular interval? Well, you can always pull up the R script and re-run the script. But wouldn’t it be nicer if it would be done automatically without you being in the middle to initiate R and running the script?&lt;/p&gt;
&lt;p&gt;In this article we’ll know how to do exactly that!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outline-of-what-well-do-here&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline of what we’ll do here&lt;/h2&gt;
&lt;p&gt;Broady, this article has two sections:&lt;/p&gt;
&lt;p&gt;Firstly, in this article we’ll create a report that uses a live data, meaning a data source that gets updated in regular interval.&lt;/p&gt;
&lt;p&gt;Secondly, once the report is created, we’ll automate the process of recreating the report daily to capture the updated data.&lt;/p&gt;
&lt;div id=&#34;part01-creating-the-report&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part01: Creating the report:&lt;/h3&gt;
&lt;p&gt;Since the goal of this article is to automate the reproduction of an already built report, I have created a report already and posted here: &lt;a href=&#34;http://rpubs.com/arafath/CRAN_Report&#34; class=&#34;uri&#34;&gt;http://rpubs.com/arafath/CRAN_Report&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feel free to go to that report and recreate it in your work station. You can name the RMarkdown script same as mine (&lt;em&gt;‘CRAN_Download_Report.Rmd’&lt;/em&gt;) and save it in the same location where you want to have your &lt;em&gt;.bat&lt;/em&gt; file and other outputs stored.&lt;/p&gt;
&lt;p&gt;What is done in that report looks like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Calling API from The Comprehensive R Archive Network (CRAN) to download daily and weekly download count of packages,&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Loading the data in R,&lt;/li&gt;
&lt;li&gt;Calculating some basic statistics e.g. counts,&lt;/li&gt;
&lt;li&gt;Visualizing the data&lt;/li&gt;
&lt;li&gt;Generating a report (html format) with the basic stats and visuals.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Data used:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;CRAN has an API calling which we can get the total number of times any package is downloaded during a specific time. We’ll use the package called &lt;em&gt;cranlogs&lt;/em&gt; to call the api.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part02-automating-the-report-reproduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part02: Automating the report reproduction&lt;/h3&gt;
&lt;p&gt;Once we have a working R script that produces result that we want, the reproduction workflow looks like as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open up R console or some IDE&lt;/li&gt;
&lt;li&gt;Load the required R script&lt;/li&gt;
&lt;li&gt;Run the script to produce the result&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this step we’ll know how to tell R to do all these above steps &lt;strong&gt;automatically&lt;/strong&gt;. In doing so R will also complete all the steps mentioned in Part01 too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How does the automation work:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To automate rerunning the R script we will use Windows Task Scheduler (WTS). Using task scheduler a user can ask windows to execute a batch file (.bat) at a regular interval. A batch file contains a series of commands that can be executed by the command line interpreter.&lt;/p&gt;
&lt;p&gt;We will create a batch file that will run an R script automatically on daily basis. Inside that R script, it’s instructed to call the .Rmd file which creates the report.&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;div id=&#34;creating-the-r-script-to-run-the-.rmd-file&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Creating the R script to run the .Rmd file&lt;/h4&gt;
&lt;p&gt;You can copy and paste the following codes in a R script and save it as &lt;em&gt;run.R&lt;/em&gt; (my r script file name):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Loading libraries [install the libraries before if not already installed]
library(knitr)
library(rmarkdown)
library(mailR)

# Knits rmd file (.Rmd is saved in the working directory)
knit(&amp;#39;CRAN_Download_Report.Rmd&amp;#39;)

# Creates the html output
render(&amp;quot;CRAN_Download_Report.md&amp;quot;)

# sending email notification
send.mail(from = &amp;quot;youremail@gmail.com&amp;quot;,
      to = c(&amp;quot;testemail@gmail.com&amp;quot;),
      cc = &amp;#39;youremail@gmail.com&amp;#39;,
      replyTo = c(&amp;quot;Reply to someone else &amp;lt;youremail@gmail.com&amp;gt;&amp;quot;),
      subject = &amp;quot;Report update status&amp;quot;,
      body = &amp;quot;Daily report on CRAN package download is updated!&amp;quot;,
      smtp = list(host.name = &amp;quot;smtp.gmail.com&amp;quot;, port = 587, user.name = &amp;quot;youremail&amp;quot;, passwd =    &amp;quot;password&amp;quot;, tls = TRUE),
      authenticate = TRUE,
      send = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What this R script does is basically kniting the rmd file and generate a html report, save it in the working directory and send an email notification.&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-r-from-windows-command-shell&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Running R from Window’s command shell&lt;/h4&gt;
&lt;p&gt;Before creating the batch file, we can run our R script from command terminal manually and check if it runs as expected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting up directory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Open the windows command shell. Search for ‘cmd’ or ‘command prompt’ in the windows and open it. It will open the black command shell.&lt;/p&gt;
&lt;p&gt;Now change the command directory to your desired location using ‘cd’ command followed by your desired file location (ref: image01).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/cmd_cd.png&#34; width=&#34;800&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;image01&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Running R from command line&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The structure of the command that we’ll use is like this:
&amp;lt;R.exe location&amp;gt; CMD BATCH &amp;lt;.R file location&amp;gt; &lt;file saving location&gt;&lt;/p&gt;
&lt;p&gt;Here,&lt;br /&gt;
- &lt;em&gt;R.exe location&lt;/em&gt; is the file location where your R executible file is located. Executing this file should open up the R console.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;.R file location&lt;/em&gt; is the file location where you have saved your r script which will call the .Rmd file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;file saving location&lt;/em&gt; is the location where you want to save your execution output.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For similiplicity, I’m using the same file location as my R working directory and location to save any outputs.&lt;/p&gt;
&lt;p&gt;These are the exact locations in my case:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;R.exe location = &amp;quot;C:\Program Files\R\R-3.6.1\bin\x64\R.exe&amp;quot; 
.R file location = &amp;quot;C:\Users\ahossa1\Personal\Learning\Automating R Script\run.R&amp;quot; 
file saving location = &amp;quot;C:\Users\ahossa1\Personal\Learning\Automating R Script\test.Rout&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the final line of code in my computer (ref: image02):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;C:\Program Files\R\R-3.6.1\bin\x64\R.exe&amp;quot; CMD BATCH &amp;quot;C:\Users\ahossa1\Personal\Learning\Projects\Automating R Script\run.R&amp;quot; &amp;quot;C:\Users\ahossa1\Personal\Learning\Projects\Automating R Script\CRAN.Rout&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/cmd_batch.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image02&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;Once you enter the command (image02) and execute it, it should run the R script, which will knit rmarkdown document and save the report. You should also receive an email with the notification!&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-batch-file-with-the-command-line-instructions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Creating a batch file with the command line instructions&lt;/h4&gt;
&lt;p&gt;We can save the command line instructions (image02) as a .bat file and save it. Then, any time we’ll need to re-create the report we can execute the .bat file and it’ll automatically call upon command line interface and execute the R script.&lt;/p&gt;
&lt;p&gt;To do that, open a text file (.txt). Paste the Windows shell commands in the .txt file and save it with an extension of &lt;em&gt;.bat&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In my computer I have named it &lt;em&gt;‘run.bat’&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;font color = maroon&gt;&lt;strong&gt;In cases where you don’t need to re-create a report on regular interval, you can just use this .bat file. All you’ll have to do is to double click (or single click) the .bat file and the report will be generated with the updated data.&lt;/strong&gt;&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automating-command-line-activities-using-windows-task-scheduler&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Automating command line activities using Windows Task Scheduler&lt;/h4&gt;
&lt;p&gt;Now we’ll ask our computer to automatically call the .bat file in regular interval.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In Windows search bar search for ‘Task Schduler’ and open the app. Below how it looks like in my computer&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/taskScheduler.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image03&#34; /&gt;
&lt;/center&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Select &lt;em&gt;Create Basic Task&lt;/em&gt; (red marked on image03).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Give the task a name&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/tsName.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image04&#34; /&gt;
&lt;/center&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go next and select a trigger. I selected &lt;em&gt;Daily&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
image05
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/tsInterval.png&#34; width=&#34;800&#34; alt=&#34;image05&#34; /&gt;
&lt;/center&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go next and select start time.&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/tsStart.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image06&#34; /&gt;
&lt;/center&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go next and select &lt;em&gt;Start a program&lt;/em&gt; as the action.&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/tsEnd.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image07&#34; /&gt;
&lt;/center&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go next and load the program/script (.bat file).&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/tsBat.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image08&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Voilà!!&lt;/strong&gt; We are done!&lt;/p&gt;
&lt;p&gt;Now every day at 3.30pm. the report below with CRAN package downloads will be created and an email notification will be sent!&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/output.png&#34; width=&#34;800&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Topic Modeling and Sentiment Analysis on Tweets</title>
      <link>/post/twitter-topic-modeling-sentiment-analysis/</link>
      <pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/twitter-topic-modeling-sentiment-analysis/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objective&#34;&gt;Objective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-collection&#34;&gt;Data collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discussion-of-the-methodology&#34;&gt;Discussion of the methodology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-processing&#34;&gt;Data processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#topic-modeling-using-lda&#34;&gt;Topic modeling using LDA&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#additional-analysis-sentiment-analysis-on-rohingya-topic&#34;&gt;Additional analysis: Sentiment analysis on Rohingya topic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overall-finding-and-discussion&#34;&gt;Overall finding and discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Twitter is a popular source for minning social media posts. In this article I harvested tweets that had mention of ‘Bangladesh’, my home country and ran two specific text analysis: topic modeling and sentiment analysis. The overall goal was to understand which topics related to Bangladesh are popular among the Twitter users and derive some understanding about the sentiments that they expressed through their tweets.&lt;/p&gt;
&lt;div id=&#34;objective&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Objective&lt;/h1&gt;
&lt;p&gt;Breaking down the objective for clear analysis gives us three specific goals:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Around which topics twitter discussions usually circle around,&lt;/li&gt;
&lt;li&gt;What is the overall sentiment about Bangladesh that is conveyed by the tweets,&lt;/li&gt;
&lt;li&gt;As an extension of the previous two steps: A topic wise sentiment analysis to reveal what kind of sentiments(s) carried by the generally discussed topics.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data collection&lt;/h1&gt;
&lt;p&gt;For that study I used public API that is provided from Twitter for twitter analysis. I fetched total 20,000 random twitter posts that were in English and had a mention of ‘Bangladesh’. So I used ‘Bangladesh’ as the search term and collected total 20,000 twitters using R through the public API of Twitter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion-of-the-methodology&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion of the methodology&lt;/h1&gt;
&lt;p&gt;To achieve this objective I applied Latent Dirichlet allocation (LDA) model from topicmodels package in R. LDA model is an unsupervised machine learning algorithm which was first presented as a topic discovery model by David Blei, Andrew Ng, and Michael I. Jordan in 2003.
LDA considers a document as a collection of topics. So each word in the document is considered as part of one or more topics. LDA clusters the words under their respective topics. As a statistical model, LDA provides probability of each word to be belonging to a topic and again a probability of each topic to be belonging to each document.&lt;br /&gt;
To run LDA, we have to pick number of topics. Since, based on this number LDA breaks down a document and words, in this study, I will try two different total numbers of topics. In LDA model, what could be the total number of topics to be looked for is a balance between granularity versus generalization. More number of topics can provide granularity but may become difficult to divide in clearly segregated topics. On the other hand, less number of topics can be overly generalized and may combine different topics into one.
On the later part for sentiment analysis lexicon based sentiment analysis approach was followed. The lexicon used was NRC Emotion Lexicon (EmoLex) which is a crowd-sourced lexicon created by Dr. Saif Mohammad, senior research officer at the National Research Council, Canada. NRC lexicon has a division of words based on 8 prototypical emotions namely: trust, surprise, sadness, joy, fear, disgust, anticipation, and anger and two sentiments: positive and negative. This NRC lexicon was used from the ‘tidytext’ package in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-processing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data processing&lt;/h1&gt;
&lt;p&gt;I have already harvested the tweets and fetched texts from the tweets into text file: ‘bd_tweets_20k.Rds’. So I will skip the initial part of coding showing fetching tweets. Rather I will start by reading the already saved file and then will show the data cleaning and processing step by step.&lt;/p&gt;
&lt;p&gt;Reading text file of the tweets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tweets = readRDS(&amp;#39;../../source_files/bd_tweets_20k.Rds&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before going into the data cleaning step couple of things are to be cleared:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It’s very important to maintain logical order in executing the cleaning commands. Other wise some information can be missed unintentionally. For example, if we convert all the tweets and later on apply ‘gsub’ command to remove retweets with ‘rt’ pattern we may lose part of words that contain ‘rt’. Retweets are marked as RT in the begining but since we converted everything into lower case using ‘tolower’ function, lateron our programs would not be able to detect difference of ‘rt’ for retweet and any other use of ‘rt’ as part of a word. Let’s say there’s a word ‘Part’, after the transormation we’ll only see ‘Pa’ and ‘rt’ part will be replaced by blank.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Throughout the cleaning step it’s a good practice to randomly check the text file to make sure no unexpected transformation takes place. For example, I will view 500th tweet from my file as a benchmark. That tweet I picked arbitrarilly. I will check text of that tweet before starting the data cleaning process and also will view at different points during the cleaning steps.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s our sample tweets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(as.character(tweets[[1500]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Half a million Rohingya refugee children at risk in overcrowded camps in Bangladesh with cyclone and… https://t.co/jrp3yEvMJN #Bangladesh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will revisit our sample tweet at different points during the next data cleaning process.&lt;/p&gt;
&lt;p&gt;In the following section I start data cleaning process by converting the text to ASCII format to get rid of the funny characters usually used in Twitter messages. Here is one thing can be noted that these funny characters may contain significant subtle information about sentiment carried by the messages but since it will extend the area covered by this report, it has been skipped here. But it could definitely be a future research area! Before going any further&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Convert to basic ASCII text to avoid silly characters
tweets &amp;lt;- iconv(tweets, to = &amp;quot;ASCII&amp;quot;, sub = &amp;quot; &amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the following code section, I will apply bunch of codes to remove special characters, hyperlink, usernames, tabs, punctuations and unnecessary white spaces. Because all these are not don’t have any relation to the topic modeling. I have mentioned specific use of each code along with the codes below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tweets &amp;lt;- gsub(&amp;quot;(RT|via)((?:\\b\\W*@\\w+)+)&amp;quot;, &amp;quot;&amp;quot;, tweets)  # Remove the &amp;quot;RT&amp;quot; (retweet) and usernames 
tweets = gsub(&amp;quot;http.+ |http.+$&amp;quot;, &amp;quot; &amp;quot;, tweets)  # Remove html links
tweets = gsub(&amp;quot;http[[:alnum:]]*&amp;quot;, &amp;quot;&amp;quot;, tweets)
tweets = gsub(&amp;quot;[[:punct:]]&amp;quot;, &amp;quot; &amp;quot;, tweets)  # Remove punctuation
tweets = gsub(&amp;quot;[ |\t]{2,}&amp;quot;, &amp;quot; &amp;quot;, tweets)  # Remove tabs
tweets = gsub(&amp;quot;^ &amp;quot;, &amp;quot;&amp;quot;, tweets)  # Leading blanks
tweets = gsub(&amp;quot; $&amp;quot;, &amp;quot;&amp;quot;, tweets)  # Lagging blanks
tweets = gsub(&amp;quot; +&amp;quot;, &amp;quot; &amp;quot;, tweets) # General spaces &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above bunch of cleaning codes we have removed html, username and so on. We saw our sample tweet had a html link in it. Let’s check if the transoformation worked properly or not:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(as.character(tweets[[1500]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Half a million Rohingya refugee children at risk in overcrowded camps in Bangladesh with cyclone and&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the punchtuations (.) and website link have been removed from our sample tweet as intended.&lt;/p&gt;
&lt;p&gt;I will convert all the tweets in lower case since in R words are case sensitive. For example: ‘Tweets’ and ‘tweets’ are considered as two different words. Moreover, I will remove the duplecate tweets. Among the tweets downloaded using twitter public API there duplicate tweets also exist. To make sure the tweets that are used here are not duplicated now I will remove the duplicated tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tweets = tolower(tweets)
tweets = unique(tweets)
writeLines(as.character(tweets[[1500]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## best quality underground metal detector in bangladesh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check I have extracted the 1500th tweet. But this time I have got a different tweet. Because after removing duplecate tweets I had left with 5,561 tweets out of 20,000 tweets which I started with. So the serial number of tweets have also changed.&lt;/p&gt;
&lt;p&gt;As the next step of data processing I will convert this tweets file, which is a character vector, into a corpus. In general term, corpus in linguistic means a structured set of texts that can be used for statistical analysis, hypothesis testing, occurance checking and validating linguistic rules. To To achive this goal I will use ‘corpus’ and ‘VectorSource’ commands from ‘tm’ library in R. While ‘VectorSource’ will interpret each element of our character vector file ‘tweets’ as a document and feed that input into ‘corpus’ command. Which eventually will convert that into corpus suitable for statistical analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tm)
corpus &amp;lt;- Corpus(VectorSource(tweets))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will do some more cleaning on the corpus by removing stop words and numbers because both these have very little value, if there is any, towards our goal of sentiment analylsis and topic modeling. For clarity I will explain a bit more on stop words here before going into coding. Stop words are some extremely common words used in a language which may carry very little value for a particular analysis. In this case I will use the stop words list comes along with ‘tm’ package. To get an idea of the list here are some example of stop words: a, about, above and so on. The exhaustive list can be found in this Github link: &lt;a href=&#34;https://github.com/arc12/Text-Mining-Weak-Signals/wiki/Standard-set-of-english-stopwords&#34; class=&#34;uri&#34;&gt;https://github.com/arc12/Text-Mining-Weak-Signals/wiki/Standard-set-of-english-stopwords&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpus &amp;lt;- tm_map(corpus, removeWords, stopwords(&amp;quot;english&amp;quot;))  
corpus &amp;lt;- tm_map(corpus, removeNumbers)
writeLines(as.character(corpus[[1500]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## best quality underground metal detector  bangladesh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see from our sample tweet that a bunch of stop words (in) is removed.&lt;/p&gt;
&lt;p&gt;At this step I will convert the words in the corpus into stem words. In general terms, word stemming means the process of reducing a word to its base form which may or may not be the same as the morphological root of the word or may or may not bear meaning by the stem word itself. For example, all these words: ‘fishing’, ‘fisheries’ can be reduced to ‘fish’ by a stemming algorithm. Here ‘fish’ bears a meaning. But on the other hand this bunch of words: ‘argue’, ‘argued’ can be reduced to ‘argu’ in this case the stem doesn’t bear any particular meaning. Stemming a document makes it easier to cluster words and make analysis since. In addition to the stemming I will also delete my search key ‘Bangladesh’ from the tweets. Since I am analyzing tweets containing Bangaldesh and ‘amp’, it’s illogical to keep the term ‘bangladesh’ since that’s the search term and ‘amp’is abbrebiation of ’Accelerated Mobile Page’ which is a part of html link that improved web surfing experience from mobile devices.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpus &amp;lt;- tm_map(corpus, stemDocument)
corpus = tm_map(corpus, removeWords, c(&amp;quot;bangladesh&amp;quot;,&amp;quot;amp&amp;quot;, &amp;quot;will&amp;quot;, &amp;#39;get&amp;#39;, &amp;#39;can&amp;#39;))

writeLines(as.character(corpus[[1500]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## best qualiti underground metal detector&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again a recheck of our sample tweet we can see ‘quality’ has been tranformed into their stem form: ‘qualiti’ and ‘bangladesh’ has been removed.&lt;/p&gt;
&lt;p&gt;I am finally done with our first step of data cleaning and pre-processing. On the next step I will start data processing to create our topic model. But before diving into model creation I decided to crate a word cloud to get a feel about the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wordcloud)
set.seed(1234)
palet  = brewer.pal(8, &amp;#39;Dark2&amp;#39;)
wordcloud(corpus, min.freq = 50, scale = c(4, 0.2) , random.order = TRUE, col = palet)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-23-twitter-sentiment-analysis-on-bangladesh_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the resulting word cloud we can see that the words are colored differently, which is based on the frequencies of the words appearing in the tweets. Looking at the most largest two fonts (black and green) we can find these words: rohingya, refuge, today, india, cricket, live. To interpret anything from such word cluster subject knowledge comes handy. Sinice I am from Bangladesh, I know that influx of Rohingya refugee from Myanmar is one of the most recent most discussed issue. Intuitively enough, Rohingya, refuge can be classified as related to the Rohingya crisis. On the other hand Cricket is the most popular game in Bangladesh along with other countries from south asian region. Cricket and Live can be thought to be related to Cricket. India and Today don’t have a general strong association with either of the two primary topics that we have sorted out. We will see how it goes in our further analysis in topic modeling.&lt;/p&gt;
&lt;p&gt;As a next processing step now I will convert our corpus in a Document Term Matrix (DTM). DTM creates a matrix that consists all words or terms as an individual column and each document, in our case each tweet, as a row. Numeric value of 1 is assigned to the words that apprear in the document from the corresponding row and value of 0 is assigned to the rest of the words in that row. Thus the resulting DTM file is a sparse which is a large matrix containing a lot of 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtm = DocumentTermMatrix(corpus)
dtm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;&amp;lt;DocumentTermMatrix (documents: 5561, terms: 8561)&amp;gt;&amp;gt;
## Non-/sparse entries: 44969/47562752
## Sparsity           : 100%
## Maximal term length: 30
## Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the file summary of ‘dtm’ file we can see that it contains total 5,561 document, which is the total number of tweets that we have, and total 8,565 term, which shows we have total 8,565 unique words in our tweets. From the non/sparse entries ratio and the percentage of Sparsity we can see that the sparsity of the file, which is not exactly 100 but very close to 100, is very very high which is means lot of words appeard only in few tweets. Let’s inspect the ‘dtm’ file to have a feel about the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doc.length = apply(dtm, 1, sum)
dtm = dtm[doc.length &amp;gt; 0,]
dtm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;&amp;lt;DocumentTermMatrix (documents: 5552, terms: 8561)&amp;gt;&amp;gt;
## Non-/sparse entries: 44969/47485703
## Sparsity           : 100%
## Maximal term length: 30
## Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspect(dtm[1:2,10:15])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;&amp;lt;DocumentTermMatrix (documents: 2, terms: 6)&amp;gt;&amp;gt;
## Non-/sparse entries: 6/6
## Sparsity           : 50%
## Maximal term length: 6
## Weighting          : term frequency (tf)
## Sample             :
##     Terms
## Docs dinesh india injur jan within year
##    1      0     0     0   0      1    1
##    2      1     1     1   1      0    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that out of the five terms first four terms were present in doc 2 and rest 2 terms were present in the doc 1. And accordingly the value of 1 and 0 have been distributed in the cells. Now let’s look at some of the most frequent words in our DTM.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
freq = colSums(as.matrix(dtm))
length(freq)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8561&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ord = order(freq, decreasing = TRUE)
freq[head(ord, n = 20)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rohingya     news  zimbabw    india   refuge     live  cricket    today 
##      590      509      465      324      308      299      297      296 
##      tri      new     camp     seri pakistan  myanmar    match   nation 
##      266      248      245      239      238      236      221      203 
##   bangla   wicket      odi    peopl 
##      201      187      182      182&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the list of 20 most frequent words we can see that Rohingya crisis and Cricket related terms are the most frequntly used terms. Which shows resemblance with what we saw in our wordcloud. We can now see how different words are associated. Since we see that Cricket and Rohingy are two frequntly used topics, we can try to see which words associate with these two words. For this we will use ‘findAssocs’ command from ‘tm’ package. To run this command we need to provde the benchmark term and then a minimum value of correlation, which can range from 0 to 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;findAssocs(dtm, &amp;quot;cricket&amp;quot;,0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $cricket
##     cup zimbabw   score    team 
##    0.23    0.22    0.20    0.20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;findAssocs(dtm, &amp;#39;rohingya&amp;#39;, 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $rohingya
##   refuge     camp  myanmar  repatri    crisi     hous children 
##     0.51     0.46     0.41     0.33     0.25     0.25     0.24&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;findAssocs(dtm, &amp;#39;news&amp;#39;, 0.2 )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $news
##   today  latest  bangla   updat januari     atn  decemb ekattor  jamuna ekushey 
##    0.77    0.77    0.76    0.68    0.56    0.53    0.34    0.33    0.28    0.26 
##    post channel 
##    0.21    0.21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From our resulting associations for both the words, we can see Cricket is associated with the words cup, zimbabwe, score and team. Which makes proper sense because every other words except Zimbabwe are related to game and Zimbabwe is one of the common team with whom Bangladesh have had quite a lot of cricket matches (such insights come from subject matter knowledge!). On the other hand, with the word Rohingya we can see assiciated words camp, refugee, myanmar, repatriation etc. evolve around the crisis created by the Rohingya refugees coming from Bangladesh’s neighboring country Myanmar.&lt;/p&gt;
&lt;p&gt;I will plot the most frequest 100 words now in a barplot to visually see how their frequencies are distributed. Checking the list of frequent words in list and graphically has two benefits: firstly, it gives a feeling about the analysis and secondly, it puts some sort of control on the quality of data cleaning done in previous steps. For example, after generating the most frequent words I found some of the words such as: amp, will, can, get are not removed. So I went back and added these words in the word remove step of data cleaning.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot = data.frame(words = names(freq), count = freq)
library(ggplot2)
plot = subset(plot, plot$count &amp;gt; 150) #creating a subset of words having more than 100 frequency
str(plot)
ggplot(data = plot, aes(words, count)) + geom_bar(stat = &amp;#39;identity&amp;#39;) + ggtitle(&amp;#39;Words used more than 150 times&amp;#39;)+coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-23-twitter-sentiment-analysis-on-bangladesh_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;topic-modeling-using-lda&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Topic modeling using LDA&lt;/h1&gt;
&lt;p&gt;I have used ‘topicmodel’ package available in R for topic modeling.
As discussed earlier, in LDA model number of topics are to be selected. Based on which LDA model creates the probability of each topic in each document and also distributes the words under each topic. Selecting more number of topics may result in more grannular segregation but at the same time the differences among different topics may get blurred. While on the other hand selecting very small number of topic can lead to losing possible topic. So to minimze this error I tried three different K or number of topics to create my LDA model. I used 2, 5, 10 as the number of topics and created three different LDA models based on these K values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(topicmodels)
#LDA model with 5 topics selected
lda_5 = LDA(dtm, k = 5, method = &amp;#39;Gibbs&amp;#39;, 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 2 topics selected
lda_2 = LDA(dtm, k = 2, method = &amp;#39;Gibbs&amp;#39;, 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
lda_10 = LDA(dtm, k = 10, method = &amp;#39;Gibbs&amp;#39;, 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;LDA model produces a good bulk of information. But getting the most frequent words under each topic and document wise probability of each topic are the two most important pieces of information that I can use for my analysis purpose. First of all I will fetch top 10 terms in each topic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Top 10 terms or words under each topic
top10terms_5 = as.matrix(terms(lda_5,10))
top10terms_2 = as.matrix(terms(lda_2,10))
top10terms_10 = as.matrix(terms(lda_10,10))

top10terms_5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Topic 1   Topic 2    Topic 3   Topic 4    Topic 5 
##  [1,] &amp;quot;news&amp;quot;    &amp;quot;rohingya&amp;quot; &amp;quot;zimbabw&amp;quot; &amp;quot;india&amp;quot;    &amp;quot;new&amp;quot;   
##  [2,] &amp;quot;live&amp;quot;    &amp;quot;refuge&amp;quot;   &amp;quot;cricket&amp;quot; &amp;quot;pakistan&amp;quot; &amp;quot;one&amp;quot;   
##  [3,] &amp;quot;today&amp;quot;   &amp;quot;camp&amp;quot;     &amp;quot;tri&amp;quot;     &amp;quot;peopl&amp;quot;    &amp;quot;year&amp;quot;  
##  [4,] &amp;quot;bangla&amp;quot;  &amp;quot;myanmar&amp;quot;  &amp;quot;seri&amp;quot;    &amp;quot;countri&amp;quot;  &amp;quot;day&amp;quot;   
##  [5,] &amp;quot;dhaka&amp;quot;   &amp;quot;girl&amp;quot;     &amp;quot;match&amp;quot;   &amp;quot;like&amp;quot;     &amp;quot;see&amp;quot;   
##  [6,] &amp;quot;januari&amp;quot; &amp;quot;children&amp;quot; &amp;quot;nation&amp;quot;  &amp;quot;time&amp;quot;     &amp;quot;just&amp;quot;  
##  [7,] &amp;quot;now&amp;quot;     &amp;quot;muslim&amp;quot;   &amp;quot;wicket&amp;quot;  &amp;quot;indian&amp;quot;   &amp;quot;work&amp;quot;  
##  [8,] &amp;quot;latest&amp;quot;  &amp;quot;say&amp;quot;      &amp;quot;odi&amp;quot;     &amp;quot;take&amp;quot;     &amp;quot;week&amp;quot;  
##  [9,] &amp;quot;updat&amp;quot;   &amp;quot;sex&amp;quot;      &amp;quot;banvzim&amp;quot; &amp;quot;don&amp;quot;      &amp;quot;follow&amp;quot;
## [10,] &amp;quot;love&amp;quot;    &amp;quot;million&amp;quot;  &amp;quot;world&amp;quot;   &amp;quot;nepal&amp;quot;    &amp;quot;last&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top10terms_2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Topic 1    Topic 2   
##  [1,] &amp;quot;rohingya&amp;quot; &amp;quot;news&amp;quot;    
##  [2,] &amp;quot;refuge&amp;quot;   &amp;quot;zimbabw&amp;quot; 
##  [3,] &amp;quot;camp&amp;quot;     &amp;quot;india&amp;quot;   
##  [4,] &amp;quot;myanmar&amp;quot;  &amp;quot;live&amp;quot;    
##  [5,] &amp;quot;peopl&amp;quot;    &amp;quot;cricket&amp;quot; 
##  [6,] &amp;quot;girl&amp;quot;     &amp;quot;today&amp;quot;   
##  [7,] &amp;quot;countri&amp;quot;  &amp;quot;tri&amp;quot;     
##  [8,] &amp;quot;children&amp;quot; &amp;quot;new&amp;quot;     
##  [9,] &amp;quot;like&amp;quot;     &amp;quot;seri&amp;quot;    
## [10,] &amp;quot;one&amp;quot;      &amp;quot;pakistan&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top10terms_10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Topic 1       Topic 2           Topic 3    Topic 4   Topic 5   Topic 6
##  [1,] &amp;quot;time&amp;quot;        &amp;quot;now&amp;quot;             &amp;quot;india&amp;quot;    &amp;quot;cricket&amp;quot; &amp;quot;zimbabw&amp;quot; &amp;quot;girl&amp;quot; 
##  [2,] &amp;quot;dhaka&amp;quot;       &amp;quot;one&amp;quot;             &amp;quot;pakistan&amp;quot; &amp;quot;world&amp;quot;   &amp;quot;tri&amp;quot;     &amp;quot;sex&amp;quot;  
##  [3,] &amp;quot;bangladeshi&amp;quot; &amp;quot;report&amp;quot;          &amp;quot;like&amp;quot;     &amp;quot;team&amp;quot;    &amp;quot;seri&amp;quot;    &amp;quot;love&amp;quot; 
##  [4,] &amp;quot;make&amp;quot;        &amp;quot;work&amp;quot;            &amp;quot;nepal&amp;quot;    &amp;quot;run&amp;quot;     &amp;quot;match&amp;quot;   &amp;quot;women&amp;quot;
##  [5,] &amp;quot;two&amp;quot;         &amp;quot;watch&amp;quot;           &amp;quot;hindus&amp;quot;   &amp;quot;canada&amp;quot;  &amp;quot;nation&amp;quot;  &amp;quot;nude&amp;quot; 
##  [6,] &amp;quot;islam&amp;quot;       &amp;quot;just&amp;quot;            &amp;quot;south&amp;quot;    &amp;quot;start&amp;quot;   &amp;quot;wicket&amp;quot;  &amp;quot;video&amp;quot;
##  [7,] &amp;quot;high&amp;quot;        &amp;quot;right&amp;quot;           &amp;quot;want&amp;quot;     &amp;quot;day&amp;quot;     &amp;quot;odi&amp;quot;     &amp;quot;kill&amp;quot; 
##  [8,] &amp;quot;state&amp;quot;       &amp;quot;indian&amp;quot;          &amp;quot;think&amp;quot;    &amp;quot;cup&amp;quot;     &amp;quot;first&amp;quot;   &amp;quot;fuck&amp;quot; 
##  [9,] &amp;quot;also&amp;quot;        &amp;quot;mishalhusainbbc&amp;quot; &amp;quot;back&amp;quot;     &amp;quot;score&amp;quot;   &amp;quot;banvzim&amp;quot; &amp;quot;porn&amp;quot; 
## [10,] &amp;quot;much&amp;quot;        &amp;quot;visit&amp;quot;           &amp;quot;take&amp;quot;     &amp;quot;play&amp;quot;    &amp;quot;win&amp;quot;     &amp;quot;dhaka&amp;quot;
##       Topic 7   Topic 8    Topic 9 Topic 10   
##  [1,] &amp;quot;news&amp;quot;    &amp;quot;rohingya&amp;quot; &amp;quot;peopl&amp;quot; &amp;quot;new&amp;quot;      
##  [2,] &amp;quot;today&amp;quot;   &amp;quot;refuge&amp;quot;   &amp;quot;year&amp;quot;  &amp;quot;countri&amp;quot;  
##  [3,] &amp;quot;live&amp;quot;    &amp;quot;camp&amp;quot;     &amp;quot;help&amp;quot;  &amp;quot;see&amp;quot;      
##  [4,] &amp;quot;bangla&amp;quot;  &amp;quot;myanmar&amp;quot;  &amp;quot;need&amp;quot;  &amp;quot;week&amp;quot;     
##  [5,] &amp;quot;januari&amp;quot; &amp;quot;children&amp;quot; &amp;quot;home&amp;quot;  &amp;quot;follow&amp;quot;   
##  [6,] &amp;quot;latest&amp;quot;  &amp;quot;muslim&amp;quot;   &amp;quot;give&amp;quot;  &amp;quot;last&amp;quot;     
##  [7,] &amp;quot;updat&amp;quot;   &amp;quot;say&amp;quot;      &amp;quot;look&amp;quot;  &amp;quot;england&amp;quot;  
##  [8,] &amp;quot;sri&amp;quot;     &amp;quot;million&amp;quot;  &amp;quot;babi&amp;quot;  &amp;quot;australia&amp;quot;
##  [9,] &amp;quot;lanka&amp;quot;   &amp;quot;support&amp;quot;  &amp;quot;sinc&amp;quot;  &amp;quot;bts&amp;quot;      
## [10,] &amp;quot;post&amp;quot;    &amp;quot;repatri&amp;quot;  &amp;quot;even&amp;quot;  &amp;quot;set&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that all three models picked the topics of Cricket and Rohingiya. But models with 5 and 10 topics also picked some other topics anlong with these two. From which we can see the application of the previous discussion about grannularity vs generalization. If we look at the top words from all the topics created from model with 10 topics, we can see that overall there is a lack of coherence among the words inside each topic. Similar observation can be made for the model with 5 topics. While the model with 2 topics provide two topics with a compact coherence among the topics. Another important thing to notice is that how the model with 10 topic picked some topic that were ignored by the model with 2 and 5 topics. Such as nudity (topic-6)!&lt;/p&gt;
&lt;p&gt;Since we can clearly see that the topics of ‘Rohingya Crisis’ and ‘Cricket’ are two most common topics, I will move with these topic for further analysis.&lt;/p&gt;
&lt;p&gt;Topics found out by our model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lda.topics_5 = as.matrix(topics(lda_5))
lda.topics_2 = as.matrix(topics(lda_2))
lda.topics_10 = as.matrix(topics(lda_10))
#write.csv(lda.topics_5,file = paste(&amp;#39;LDAGibbs&amp;#39;,5,&amp;#39;DocsToTopics.csv&amp;#39;))
#write.csv(lda.topics_2,file = paste(&amp;#39;LDAGibbs&amp;#39;,2,&amp;#39;DocsToTopics.csv&amp;#39;))
#write.csv(lda.topics_10,file = paste(&amp;#39;LDAGibbs&amp;#39;,10,&amp;#39;DocsToTopics.csv&amp;#39;))

summary(as.factor(lda.topics_5[,1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    1    2    3    4    5 
## 1208 1293 1230 1003  818&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(as.factor(lda.topics_2[,1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    1    2 
## 3151 2401&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(as.factor(lda.topics_10[,1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   1   2   3   4   5   6   7   8   9  10 
## 755 659 607 577 708 546 385 593 364 358&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also get document wise probability of each topic. I have created three files for each of my model and also saved the output for future use. Probability of each topic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;topicprob_5 = as.matrix(lda_5@gamma)
topicprob_2 = as.matrix(lda_2@gamma)
topicprob_10 = as.matrix(lda_10@gamma)

#write.csv(topicprob_5, file = paste(&amp;#39;LDAGibbs&amp;#39;, 5, &amp;#39;DoctToTopicProb.csv&amp;#39;))
#write.csv(topicprob_2, file = paste(&amp;#39;LDAGibbs&amp;#39;, 2, &amp;#39;DoctToTopicProb.csv&amp;#39;))
#write.csv(topicprob_10, file = paste(&amp;#39;LDAGibbs&amp;#39;, 10, &amp;#39;DoctToTopicProb.csv&amp;#39;))

head(topicprob_2,1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 0.5409836 0.4590164&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a sample we can see that according to my model with 5 topics, how document 1 has different probabilities of containing each topic. The highest probability from topic-2. Accordingly the model classified as representing topic-2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)
library(dplyr)
library(ggplot2)

#Tokenizing character vector file &amp;#39;tweets&amp;#39;.
token = data.frame(text=tweets, stringsAsFactors = FALSE) %&amp;gt;% unnest_tokens(word, text)

#Matching sentiment words from the &amp;#39;NRC&amp;#39; sentiment lexicon
senti = inner_join(token, get_sentiments(&amp;quot;nrc&amp;quot;)) %&amp;gt;%
  count(sentiment)
senti$percent = (senti$n/sum(senti$n))*100

#Plotting the sentiment summary 
ggplot(senti, aes(sentiment, percent)) +   
        geom_bar(aes(fill = sentiment), position = &amp;#39;dodge&amp;#39;, stat = &amp;#39;identity&amp;#39;)+ 
        ggtitle(&amp;quot;Sentiment analysis based on lexicon: &amp;#39;NRC&amp;#39;&amp;quot;)+
  coord_flip() +
        theme(legend.position = &amp;#39;none&amp;#39;, plot.title = element_text(size=18, face = &amp;#39;bold&amp;#39;),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face=&amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-23-twitter-sentiment-analysis-on-bangladesh_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;additional-analysis-sentiment-analysis-on-rohingya-topic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Additional analysis: Sentiment analysis on Rohingya topic&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tm)
library(quanteda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Package version: 2.0.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parallel computing: 2 of 4 threads used.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See https://quanteda.io for tutorials and examples.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;quanteda&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:tm&amp;#39;:
## 
##     as.DocumentTermMatrix, stopwords&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:NLP&amp;#39;:
## 
##     meta, meta&amp;lt;-&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:utils&amp;#39;:
## 
##     View&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpus_roh = corpus(tweets)
corpus_roh = (corpus_roh = subset(corpus_roh, grepl(&amp;#39;rohingya&amp;#39;, texts(corpus_roh))))
writeLines(as.character(corpus_roh[[150]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## the suffering in the rohingya refugee camp in bangladesh is indescribable&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Tokenizing character vector file &amp;#39;tweets&amp;#39;.
library(tidytext)
token_roh = tibble(text=corpus_roh)  %&amp;gt;% unnest_tokens(word, text, format = &amp;quot;text&amp;quot;)
 
#Matching sentiment words from the &amp;#39;NRC&amp;#39; sentiment lexicon
library(dplyr)
senti_roh = inner_join(token_roh, get_sentiments(&amp;quot;nrc&amp;quot;)) %&amp;gt;%
  count(sentiment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;word&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;senti_roh$percent = (senti_roh$n/sum(senti_roh$n))*100

#Plotting the sentiment summary 
library(ggplot2)
ggplot(senti_roh, aes(sentiment, percent)) +   
        geom_bar(aes(fill = sentiment), position = &amp;#39;dodge&amp;#39;, stat = &amp;#39;identity&amp;#39;)+ 
        ggtitle(&amp;quot;Sentiment analysis based on lexicon: &amp;#39;NRC&amp;#39;&amp;quot;)+
  coord_flip() +
        theme(legend.position = &amp;#39;none&amp;#39;, plot.title = element_text(size=18, face = &amp;#39;bold&amp;#39;),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face=&amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-23-twitter-sentiment-analysis-on-bangladesh_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;overall-finding-and-discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overall finding and discussion&lt;/h1&gt;
&lt;p&gt;From our random walk on the tweets related to Bangladesh, we have seen ‘cricket’ and ‘Rohingya’ were the two areas that people cared most. Overall, people exuded a positive sentiment along with emotions of trust and anticipation. But in case of Rohingya crisis, we showed a mixed sentiment. About Rohingya issue both the positive and negative sentiments were high. Moreover, the emotions also seemed to be mixed. People felt sorry for the Rohingya people but they also expressed fear. So, what could that mean? Were people sympathetic to the plight of Rohingya but also had some share of fear related to the issue? This study doesn’t allow us to conclude on any such conclusion. But it gives us an idea of what we may achieve by having such a walk. Maybe we need to go for a walk with Bangladesh on the online social media sites more often to get a clearer image on what people talk about Bangladesh and what may needs to be improved to leave a better digital footprint for the country.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
