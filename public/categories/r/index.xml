<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Curious Joe</title>
    <link>/categories/r/</link>
    <description>Recent content in R on Curious Joe</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Automate Your Repetitive Reports!</title>
      <link>/post/automate-your-repetitive-reports/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/automate-your-repetitive-reports/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-r-script-automation-and-why&#34;&gt;What is R script automation and why?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#outline-of-what-well-do-here&#34;&gt;Outline of what we’ll do here&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#part01-creating-the-report&#34;&gt;Part01: Creating the report:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part02-automating-the-report-reproduction&#34;&gt;Part02: Automating the report reproduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;what-is-r-script-automation-and-why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is R script automation and why?&lt;/h2&gt;
&lt;p&gt;In most use cases R is used to analyze data, run statistical tests and build model. In doing so, data scientists constantly interact with R by writing codes and produce results from these interactions. Eventually, these results are stored, shared or presented as a report. But what if you have to reproduce the report every day or in other type of regular interval? Well, you can always pull up the R script and re-run the script. But wouldn’t it be nicer if it would be done automatically without you being in the middle to initiate R and running the script?&lt;/p&gt;
&lt;p&gt;In this article we’ll know how to do exactly that!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outline-of-what-well-do-here&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline of what we’ll do here&lt;/h2&gt;
&lt;p&gt;Broady, this article has two sections:&lt;/p&gt;
&lt;p&gt;Firstly, in this article we’ll create a report that uses a live data, meaning a data source that gets updated in regular interval.&lt;/p&gt;
&lt;p&gt;Secondly, once the report is created, we’ll automate the process of recreating the report daily to capture the updated data.&lt;/p&gt;
&lt;div id=&#34;part01-creating-the-report&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part01: Creating the report:&lt;/h3&gt;
&lt;p&gt;Since the goal of this article is to automate the reproduction of an already built report, I have created a report already and posted here: &lt;a href=&#34;http://rpubs.com/arafath/CRAN_Report&#34; class=&#34;uri&#34;&gt;http://rpubs.com/arafath/CRAN_Report&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feel free to go to that report and recreate it in your work station. You can name the RMarkdown script same as mine (&lt;em&gt;‘CRAN_Download_Report.Rmd’&lt;/em&gt;) and save it in the same location where you want to have your &lt;em&gt;.bat&lt;/em&gt; file and other outputs stored.&lt;/p&gt;
&lt;p&gt;What is done in that report looks like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Calling API from The Comprehensive R Archive Network (CRAN) to download daily and weekly download count of packages,&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Loading the data in R,&lt;/li&gt;
&lt;li&gt;Calculating some basic statistics e.g. counts,&lt;/li&gt;
&lt;li&gt;Visualizing the data&lt;/li&gt;
&lt;li&gt;Generating a report (html format) with the basic stats and visuals.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Data used:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;CRAN has an API calling which we can get the total number of times any package is downloaded during a specific time. We’ll use the package called &lt;em&gt;cranlogs&lt;/em&gt; to call the api.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part02-automating-the-report-reproduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part02: Automating the report reproduction&lt;/h3&gt;
&lt;p&gt;Once we have a working R script that produces result that we want, the reproduction workflow looks like as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open up R console or some IDE&lt;/li&gt;
&lt;li&gt;Load the required R script&lt;/li&gt;
&lt;li&gt;Run the script to produce the result&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this step we’ll know how to tell R to do all these above steps &lt;strong&gt;automatically&lt;/strong&gt;. In doing so R will also complete all the steps mentioned in Part01 too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How does the automation work:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To automate rerunning the R script we will use Windows Task Scheduler (WTS). Using task scheduler a user can ask windows to execute a batch file (.bat) at a regular interval. A batch file contains a series of commands that can be executed by the command line interpreter.&lt;/p&gt;
&lt;p&gt;We will create a batch file that will run an R script automatically on daily basis. Inside that R script, it’s instructed to call the .Rmd file which creates the report.&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;div id=&#34;creating-the-r-script-to-run-the-.rmd-file&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Creating the R script to run the .Rmd file&lt;/h4&gt;
&lt;p&gt;You can copy and paste the following codes in a R script and save it as &lt;em&gt;run.R&lt;/em&gt; (my r script file name):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Loading libraries [install the libraries before if not already installed]
library(knitr)
library(rmarkdown)
library(mailR)

# Knits rmd file (.Rmd is saved in the working directory)
knit(&amp;#39;CRAN_Download_Report.Rmd&amp;#39;)

# Creates the html output
render(&amp;quot;CRAN_Download_Report.md&amp;quot;)

# sending email notification
send.mail(from = &amp;quot;youremail@gmail.com&amp;quot;,
      to = c(&amp;quot;testemail@gmail.com&amp;quot;),
      cc = &amp;#39;youremail@gmail.com&amp;#39;,
      replyTo = c(&amp;quot;Reply to someone else &amp;lt;youremail@gmail.com&amp;gt;&amp;quot;),
      subject = &amp;quot;Report update status&amp;quot;,
      body = &amp;quot;Daily report on CRAN package download is updated!&amp;quot;,
      smtp = list(host.name = &amp;quot;smtp.gmail.com&amp;quot;, port = 587, user.name = &amp;quot;youremail&amp;quot;, passwd =    &amp;quot;password&amp;quot;, tls = TRUE),
      authenticate = TRUE,
      send = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What this R script does is basically kniting the rmd file and generate a html report, save it in the working directory and send an email notification.&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-r-from-windows-command-shell&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Running R from Window’s command shell&lt;/h4&gt;
&lt;p&gt;Before creating the batch file, we can run our R script from command terminal manually and check if it runs as expected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting up directory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Open the windows command shell. Search for ‘cmd’ or ‘command prompt’ in the windows and open it. It will open the black command shell.&lt;/p&gt;
&lt;p&gt;Now change the command directory to your desired location using ‘cd’ command followed by your desired file location (ref: image01).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/cmd_cd.png&#34; width=&#34;800&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;image01&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Running R from command line&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The structure of the command that we’ll use is like this:
&amp;lt;R.exe location&amp;gt; CMD BATCH &amp;lt;.R file location&amp;gt; &lt;file saving location&gt;&lt;/p&gt;
&lt;p&gt;Here,&lt;br /&gt;
- &lt;em&gt;R.exe location&lt;/em&gt; is the file location where your R executible file is located. Executing this file should open up the R console.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;.R file location&lt;/em&gt; is the file location where you have saved your r script which will call the .Rmd file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;file saving location&lt;/em&gt; is the location where you want to save your execution output.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For similiplicity, I’m using the same file location as my R working directory and location to save any outputs.&lt;/p&gt;
&lt;p&gt;These are the exact locations in my case:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;R.exe location = &amp;quot;C:\Program Files\R\R-3.6.1\bin\x64\R.exe&amp;quot; 
.R file location = &amp;quot;C:\Users\ahossa1\Personal\Learning\Automating R Script\run.R&amp;quot; 
file saving location = &amp;quot;C:\Users\ahossa1\Personal\Learning\Automating R Script\test.Rout&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the final line of code in my computer (ref: image02):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;C:\Program Files\R\R-3.6.1\bin\x64\R.exe&amp;quot; CMD BATCH &amp;quot;C:\Users\ahossa1\Personal\Learning\Projects\Automating R Script\run.R&amp;quot; &amp;quot;C:\Users\ahossa1\Personal\Learning\Projects\Automating R Script\CRAN.Rout&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/cmd_batch.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image02&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;Once you enter the command (image02) and execute it, it should run the R script, which will knit rmarkdown document and save the report. You should also receive an email with the notification!&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-batch-file-with-the-command-line-instructions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Creating a batch file with the command line instructions&lt;/h4&gt;
&lt;p&gt;We can save the command line instructions (image02) as a .bat file and save it. Then, any time we’ll need to re-create the report we can execute the .bat file and it’ll automatically call upon command line interface and execute the R script.&lt;/p&gt;
&lt;p&gt;To do that, open a text file (.txt). Paste the Windows shell commands in the .txt file and save it with an extension of &lt;em&gt;.bat&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In my computer I have named it &lt;em&gt;‘run.bat’&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;font color = maroon&gt;&lt;strong&gt;In cases where you don’t need to re-create a report on regular interval, you can just use this .bat file. All you’ll have to do is to double click (or single click) the .bat file and the report will be generated with the updated data.&lt;/strong&gt;&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automating-command-line-activities-using-windows-task-scheduler&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Automating command line activities using Windows Task Scheduler&lt;/h4&gt;
&lt;p&gt;Now we’ll ask our computer to automatically call the .bat file in regular interval.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In Windows search bar search for ‘Task Schduler’ and open the app. Below how it looks like in my computer&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/taskScheduler.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image03&#34; /&gt;
&lt;/center&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Select &lt;em&gt;Create Basic Task&lt;/em&gt; (red marked on image03).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Give the task a name&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/tsName.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image04&#34; /&gt;
&lt;/center&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go next and select a trigger. I selected &lt;em&gt;Daily&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
image05
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/tsInterval.png&#34; width=&#34;800&#34; alt=&#34;image05&#34; /&gt;
&lt;/center&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go next and select start time.&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/tsStart.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image06&#34; /&gt;
&lt;/center&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go next and select &lt;em&gt;Start a program&lt;/em&gt; as the action.&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/tsEnd.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image07&#34; /&gt;
&lt;/center&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go next and load the program/script (.bat file).&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/tsBat.png&#34; title=&#34;fig:&#34; width=&#34;800&#34; alt=&#34;image08&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;
 
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Voilà!!&lt;/strong&gt; We are done!&lt;/p&gt;
&lt;p&gt;Now every day at 3.30pm. the report below with CRAN package downloads will be created and an email notification will be sent!&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/post/2020-06-23-automate-your-repetitive-reports_files/output.png&#34; width=&#34;800&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is that Red Wine Good Enough?</title>
      <link>/post/is-the-red-wine-good-enough/</link>
      <pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/is-the-red-wine-good-enough/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-data&#34;&gt;Exploring Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-features&#34;&gt;Exploring Features&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#transforming-target-feature&#34;&gt;Transforming Target Feature&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-predictors-visually&#34;&gt;Exploring Predictors Visually&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#checking-correlation&#34;&gt;Checking Correlation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering&#34;&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fitting-model&#34;&gt;Fitting Model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#splitting-data&#34;&gt;Splitting Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fitting-model-on-training-data&#34;&gt;Fitting Model on Training Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#checking-model-performance&#34;&gt;Checking Model Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-inisght&#34;&gt;Summary Inisght&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Let’s assume that we have been hired by a winery to build a predictive model to check the qulity of their red wine. The traiditional way of wine testing is done by a human expert. Thus the process is prone to human error. The goal is to establish a process of producing an objective method of wine testing and combining that with the existing process to reduce human error.&lt;/p&gt;
&lt;p&gt;For the purpose of building the predictive model, we’ll use a dataset provided by UCI machine learning repository. We’ll try to predict wine quality based on features associated with wine.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore the data&lt;/li&gt;
&lt;li&gt;Predict the wine quality (binary classification)&lt;/li&gt;
&lt;li&gt;Explore model result&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;exploring-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exploring Data&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Loading data, libraries and primary glimpsing over data&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# libraries
library(dplyr)
library(ggplot2)
library(caTools)
library(caret)
library(GGally)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataFrame = read.csv(&amp;quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&amp;quot;, sep = &amp;#39;;&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(dataFrame)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  fixed.acidity   volatile.acidity  citric.acid    residual.sugar  
##  Min.   : 4.60   Min.   :0.1200   Min.   :0.000   Min.   : 0.900  
##  1st Qu.: 7.10   1st Qu.:0.3900   1st Qu.:0.090   1st Qu.: 1.900  
##  Median : 7.90   Median :0.5200   Median :0.260   Median : 2.200  
##  Mean   : 8.32   Mean   :0.5278   Mean   :0.271   Mean   : 2.539  
##  3rd Qu.: 9.20   3rd Qu.:0.6400   3rd Qu.:0.420   3rd Qu.: 2.600  
##  Max.   :15.90   Max.   :1.5800   Max.   :1.000   Max.   :15.500  
##    chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      
##  Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  
##  1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956  
##  Median :0.07900   Median :14.00       Median : 38.00       Median :0.9968  
##  Mean   :0.08747   Mean   :15.87       Mean   : 46.47       Mean   :0.9967  
##  3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9978  
##  Max.   :0.61100   Max.   :72.00       Max.   :289.00       Max.   :1.0037  
##        pH          sulphates         alcohol         quality     
##  Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000  
##  1st Qu.:3.210   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  
##  Median :3.310   Median :0.6200   Median :10.20   Median :6.000  
##  Mean   :3.311   Mean   :0.6581   Mean   :10.42   Mean   :5.636  
##  3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  
##  Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the features we see ‘quality’ is our target feature. And we have total 11 features to be used as the predictors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-features&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exploring Features&lt;/h1&gt;
&lt;div id=&#34;transforming-target-feature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transforming Target Feature&lt;/h2&gt;
&lt;p&gt;Since we will cover talk about the classification model, we’ll convert our target feature from continuous to binary class. So that we would be able to fit one of the very widely used yet very easy classification models.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Distribution of original target feature labels&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# checking ratio of different labels in target feature
prop.table(table(dataFrame$quality))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##           3           4           5           6           7           8 
## 0.006253909 0.033145716 0.425891182 0.398999375 0.124452783 0.011257036&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataFrame = dataFrame %&amp;gt;%
  mutate(quality_bin = as.factor(ifelse(quality &amp;lt;= 5, 0,1))) %&amp;gt;%
  select(-quality)


p = round(prop.table(table(dataFrame$quality_bin))*100,2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After tranformation we have 53.47% cases classified records as good wines vs 46.53% as bad wines.&lt;/p&gt;
&lt;p&gt;We have a nice distribution of our target classes here! Which is very nice. Otherwise, we would’ve had to deal with &lt;em&gt;Data Balancing&lt;/em&gt;. Though we won’t cover that area in this tutorial, it’s a great discussion area to delve into. So some extra points for those who’ll learn about it!&lt;/p&gt;
&lt;p&gt;In short, we would like to &lt;strong&gt;have a balanced distribution of observations from different labels in our target feature&lt;/strong&gt;. Otherwise, some ML algorithms tend to overfit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-predictors-visually&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring Predictors Visually&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Exploring acidity&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataFrame %&amp;gt;%
  ggplot(aes(x = as.factor(quality_bin), y = fixed.acidity, color = quality_bin)) +
  geom_boxplot(outlier.color = &amp;quot;darkred&amp;quot;, notch = FALSE) +
  ylab(&amp;quot;Acidity&amp;quot;) + xlab(&amp;quot;Quality (1 = good, 2 = bad)&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;, axis.title.x = element_blank()) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/viz_acidity-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have multiple features that are continuous and can plot them similarly. Which means we’ll have to re write the code that we have just wrote in code chunk: viz_acidity again and again. In coding, we don’t want to do that. So we’ll create a function and wrap that around our code so that it can be reused in future!&lt;/p&gt;
&lt;p&gt;If it sounds too much, just stick with it. Once you see the code, it’ll make a lot more sense.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# boxplot_viz
# plots continuous feature in boxplot categorized on the quality_bin feature labels from dataFrame 
# @param feat Feature name (string) to be plotted
boxplot_viz = function(feat){

  dataFrame %&amp;gt;%
    ggplot(aes_string(x = as.factor(&amp;#39;quality_bin&amp;#39;), y = feat, color = &amp;#39;quality_bin&amp;#39;)) +
    geom_boxplot(outlier.color = &amp;quot;darkred&amp;quot;, notch = FALSE) +
    labs(title = paste0(&amp;quot;Boxplot of feature: &amp;quot;, feat)) + ylab(feat) + xlab(&amp;quot;Quality (1 = good, 2 = bad)&amp;quot;) + 
    theme(legend.position = &amp;quot;none&amp;quot;, axis.title.x = element_blank()) + 
    theme_minimal()
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot_viz(&amp;#39;volatile.acidity&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in names(dataFrame %&amp;gt;% select(-&amp;#39;quality_bin&amp;#39;))){
  print(boxplot_viz(i))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-5.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-6.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-7.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-8.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-9.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-10.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/unnamed-chunk-5-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Checking Correlation&lt;/h2&gt;
&lt;p&gt;We can quickly check correlations among our predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataFrame %&amp;gt;% 
  # correlation plot 
  ggcorr(method = c(&amp;#39;complete.obs&amp;#39;,&amp;#39;pearson&amp;#39;), 
         nbreaks = 6, digits = 3, palette = &amp;quot;RdGy&amp;quot;, label = TRUE, label_size = 3, 
         label_color = &amp;quot;white&amp;quot;, label_round = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/correlation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Highly correlated features don’t add new information to the model and blurrs the effect of individual feature on the predictor and thus makes it difficult to explain effect of individual features on target feature. This problem is called &lt;strong&gt;Multicollinearity&lt;/strong&gt;. As a general rule, we don’t want to keep features with very high correlation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What should be the threshold of correlation?&lt;/li&gt;
&lt;li&gt;How do we decide which variable to drop?&lt;/li&gt;
&lt;li&gt;Do correlated features hurt predictive accuracy?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these are great questions and worth having a good understanding about. So again extra points for those who’ll learn about !&lt;/p&gt;
&lt;p&gt;Before making any decision based on correlation, check distribution of the feature. Unless any two features have a linear relation, correlation doesn’t mean much.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Feature Engineering&lt;/h1&gt;
&lt;p&gt;Based on the insight gained from the data exploration, some features may need to be transformed or new features can be created. Some common feature engineering tasks are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normalization and standardization of features&lt;/li&gt;
&lt;li&gt;Binning continuous features&lt;/li&gt;
&lt;li&gt;Creating composit features&lt;/li&gt;
&lt;li&gt;Creating dummy variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial won’t cover &lt;em&gt;feature engineering&lt;/em&gt; but it’s a great area to explore. A great data exploration followed by necessary feature engineering are the absolute necessary prerequisites before fitting any predictive model!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting Model&lt;/h1&gt;
&lt;div id=&#34;splitting-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splitting Data&lt;/h2&gt;
&lt;p&gt;In practical world we train our predictive models on historical data which is called &lt;strong&gt;Training Data&lt;/strong&gt;. Then we apply that model on new unseen data, called &lt;strong&gt;Test Data&lt;/strong&gt;, and measure the performance. thus we can be sure that our model is stable or not over fitted on training data. But since we won’t have access to new wine data, we’ll split our dataset into training and testing data on a 80:20 ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
split = sample.split(dataFrame$quality_bin, SplitRatio = 0.80)
training_set = subset(dataFrame, split == TRUE)
test_set = subset(dataFrame, split == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check the data balance in training and test data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop.table(table(training_set$quality_bin))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##         0         1 
## 0.4652072 0.5347928&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop.table(table(test_set$quality_bin))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##        0        1 
## 0.465625 0.534375&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-model-on-training-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting Model on Training Data&lt;/h2&gt;
&lt;p&gt;We’ll fit &lt;strong&gt;Logistic Regression&lt;/strong&gt; classification model on our dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_log = glm(quality_bin ~ ., 
                data = training_set, family = &amp;#39;binomial&amp;#39;)
summary(model_log)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = quality_bin ~ ., family = &amp;quot;binomial&amp;quot;, data = training_set)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.3688  -0.8309   0.2989   0.8109   2.4184  
## 
## Coefficients:
##                        Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)           17.369521  90.765368   0.191  0.84824    
## fixed.acidity          0.069510   0.112062   0.620  0.53507    
## volatile.acidity      -3.602258   0.558889  -6.445 1.15e-10 ***
## citric.acid           -1.543276   0.638161  -2.418  0.01559 *  
## residual.sugar         0.012106   0.060364   0.201  0.84106    
## chlorides             -4.291590   1.758614  -2.440  0.01467 *  
## free.sulfur.dioxide    0.027452   0.009293   2.954  0.00314 ** 
## total.sulfur.dioxide  -0.016723   0.003229  -5.180 2.22e-07 ***
## density              -23.425390  92.700349  -0.253  0.80050    
## pH                    -0.977906   0.828710  -1.180  0.23799    
## sulphates              3.070254   0.532655   5.764 8.21e-09 ***
## alcohol                0.946654   0.120027   7.887 3.10e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1766.9  on 1278  degrees of freedom
## Residual deviance: 1301.4  on 1267  degrees of freedom
## AIC: 1325.4
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s plot the variables with the lowest p values/highest absolute z value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p = varImp(model_log) %&amp;gt;% data.frame() 
p = p %&amp;gt;% mutate(Features = rownames(p)) %&amp;gt;% arrange(desc(Overall)) %&amp;gt;% mutate(Features = tolower(Features))

p %&amp;gt;% ggplot(aes(x = reorder(Features, Overall), y = Overall)) + geom_col(width = .50, fill = &amp;#39;darkred&amp;#39;) + coord_flip() + 
  labs(title = &amp;quot;Importance of Features&amp;quot;, subtitle = &amp;quot;Based on the value of individual z score&amp;quot;) +
  xlab(&amp;quot;Features&amp;quot;) + ylab(&amp;quot;Abs. Z Score&amp;quot;) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-20-is-the-red-wine-good-enough_files/figure-html/featImp-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-model-performance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Checking Model Performance&lt;/h1&gt;
&lt;p&gt;We’ll check how our model performs by running it on our previously unseen test data. We’ll compare the predicted outcome with the actual outcome and calculate some typically used binary classification model performance measuring metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predict target feature in test data
y_pred = as.data.frame(predict(model_log, type = &amp;quot;response&amp;quot;, newdata = test_set)) %&amp;gt;% 
  structure( names = c(&amp;quot;pred_prob&amp;quot;)) %&amp;gt;%
  mutate(pred_cat = as.factor(ifelse(pred_prob &amp;gt; 0.5, &amp;quot;1&amp;quot;, &amp;quot;0&amp;quot;))) %&amp;gt;% 
  mutate(actual_cat = test_set$quality_bin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p = confusionMatrix(y_pred$pred_cat, y_pred$actual_cat, positive = &amp;quot;1&amp;quot;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 108  46
##          1  41 125
##                                           
##                Accuracy : 0.7281          
##                  95% CI : (0.6758, 0.7761)
##     No Information Rate : 0.5344          
##     P-Value [Acc &amp;gt; NIR] : 9.137e-13       
##                                           
##                   Kappa : 0.4548          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 0.668           
##                                           
##             Sensitivity : 0.7310          
##             Specificity : 0.7248          
##          Pos Pred Value : 0.7530          
##          Neg Pred Value : 0.7013          
##              Prevalence : 0.5344          
##          Detection Rate : 0.3906          
##    Detection Prevalence : 0.5188          
##       Balanced Accuracy : 0.7279          
##                                           
##        &amp;#39;Positive&amp;#39; Class : 1               
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;font color = maroon&gt;&lt;strong&gt;Model Perfomance Summary:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;: 72.81% of the wine samples have been classified correctly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensitivity/Recall&lt;/strong&gt;: 73.1% of the actual good wine samples have been classified correctly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pos Pred Value/Precision&lt;/strong&gt;: 75.3% of the total good wine predictions are actually good wines. &lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-inisght&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary Inisght&lt;/h1&gt;
&lt;p&gt;So let’s summarize about what we have learned about wine testing from our exercise:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alchohol content, Volatile Acidity, Sulphate and total Sulpher Dioxide are the top four most statistically significant features that affect wine quality.&lt;/li&gt;
&lt;li&gt;Given the information about the 11 features that we have analyzed, we can accurately predict wine quality in about 73% of the cases,&lt;/li&gt;
&lt;li&gt;Which is about 26% more accurate than the accuracy achieved by using traditional expert based method.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;a href=&#34;https://www.theguardian.com/lifeandstyle/2013/jun/23/wine-tasting-junk-science-analysis&#34;&gt;“People could tell the difference between wines under £5 and those above £10 only 53% of the time for whites and only 47% of the time for reds.”&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;center&gt;
&lt;div&gt;
&lt;p&gt;&lt;img src=&#34;https://media1.tenor.com/images/f050405657f9ac48d3b976051436e885/tenor.gif?itemid=10577278&#34; width=&#34;400&#34;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;div class=&#34;tocify-extend-page&#34; data-unique=&#34;tocify-extend-page&#34; style=&#34;height: 0;&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Topic Modeling and Sentiment Analysis on Tweets</title>
      <link>/post/twitter-topic-modeling-sentiment-analysis/</link>
      <pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/twitter-topic-modeling-sentiment-analysis/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objective&#34;&gt;Objective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-collection&#34;&gt;Data collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discussion-of-the-methodology&#34;&gt;Discussion of the methodology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-processing&#34;&gt;Data processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#topic-modeling-using-lda&#34;&gt;Topic modeling using LDA&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#additional-analysis-sentiment-analysis-on-rohingya-topic&#34;&gt;Additional analysis: Sentiment analysis on Rohingya topic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overall-finding-and-discussion&#34;&gt;Overall finding and discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Twitter is a popular source for minning social media posts. In this article I harvested tweets that had mention of ‘Bangladesh’, my home country and ran two specific text analysis: topic modeling and sentiment analysis. The overall goal was to understand which topics related to Bangladesh are popular among the Twitter users and derive some understanding about the sentiments that they expressed through their tweets.&lt;/p&gt;
&lt;div id=&#34;objective&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Objective&lt;/h1&gt;
&lt;p&gt;Breaking down the objective for clear analysis gives us three specific goals:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Around which topics twitter discussions usually circle around,&lt;/li&gt;
&lt;li&gt;What is the overall sentiment about Bangladesh that is conveyed by the tweets,&lt;/li&gt;
&lt;li&gt;As an extension of the previous two steps: A topic wise sentiment analysis to reveal what kind of sentiments(s) carried by the generally discussed topics.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data collection&lt;/h1&gt;
&lt;p&gt;For that study I used public API that is provided from Twitter for twitter analysis. I fetched total 20,000 random twitter posts that were in English and had a mention of ‘Bangladesh’. So I used ‘Bangladesh’ as the search term and collected total 20,000 twitters using R through the public API of Twitter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion-of-the-methodology&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion of the methodology&lt;/h1&gt;
&lt;p&gt;To achieve this objective I applied Latent Dirichlet allocation (LDA) model from topicmodels package in R. LDA model is an unsupervised machine learning algorithm which was first presented as a topic discovery model by David Blei, Andrew Ng, and Michael I. Jordan in 2003.
LDA considers a document as a collection of topics. So each word in the document is considered as part of one or more topics. LDA clusters the words under their respective topics. As a statistical model, LDA provides probability of each word to be belonging to a topic and again a probability of each topic to be belonging to each document.&lt;br /&gt;
To run LDA, we have to pick number of topics. Since, based on this number LDA breaks down a document and words, in this study, I will try two different total numbers of topics. In LDA model, what could be the total number of topics to be looked for is a balance between granularity versus generalization. More number of topics can provide granularity but may become difficult to divide in clearly segregated topics. On the other hand, less number of topics can be overly generalized and may combine different topics into one.
On the later part for sentiment analysis lexicon based sentiment analysis approach was followed. The lexicon used was NRC Emotion Lexicon (EmoLex) which is a crowd-sourced lexicon created by Dr. Saif Mohammad, senior research officer at the National Research Council, Canada. NRC lexicon has a division of words based on 8 prototypical emotions namely: trust, surprise, sadness, joy, fear, disgust, anticipation, and anger and two sentiments: positive and negative. This NRC lexicon was used from the ‘tidytext’ package in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-processing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data processing&lt;/h1&gt;
&lt;p&gt;I have already harvested the tweets and fetched texts from the tweets into text file: ‘bd_tweets_20k.Rds’. So I will skip the initial part of coding showing fetching tweets. Rather I will start by reading the already saved file and then will show the data cleaning and processing step by step.&lt;/p&gt;
&lt;p&gt;Reading text file of the tweets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tweets = readRDS(&amp;#39;../../source_files/bd_tweets_20k.Rds&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before going into the data cleaning step couple of things are to be cleared:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It’s very important to maintain logical order in executing the cleaning commands. Other wise some information can be missed unintentionally. For example, if we convert all the tweets and later on apply ‘gsub’ command to remove retweets with ‘rt’ pattern we may lose part of words that contain ‘rt’. Retweets are marked as RT in the begining but since we converted everything into lower case using ‘tolower’ function, lateron our programs would not be able to detect difference of ‘rt’ for retweet and any other use of ‘rt’ as part of a word. Let’s say there’s a word ‘Part’, after the transormation we’ll only see ‘Pa’ and ‘rt’ part will be replaced by blank.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Throughout the cleaning step it’s a good practice to randomly check the text file to make sure no unexpected transformation takes place. For example, I will view 500th tweet from my file as a benchmark. That tweet I picked arbitrarilly. I will check text of that tweet before starting the data cleaning process and also will view at different points during the cleaning steps.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s our sample tweets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(as.character(tweets[[1500]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Half a million Rohingya refugee children at risk in overcrowded camps in Bangladesh with cyclone and… https://t.co/jrp3yEvMJN #Bangladesh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will revisit our sample tweet at different points during the next data cleaning process.&lt;/p&gt;
&lt;p&gt;In the following section I start data cleaning process by converting the text to ASCII format to get rid of the funny characters usually used in Twitter messages. Here is one thing can be noted that these funny characters may contain significant subtle information about sentiment carried by the messages but since it will extend the area covered by this report, it has been skipped here. But it could definitely be a future research area! Before going any further&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Convert to basic ASCII text to avoid silly characters
tweets &amp;lt;- iconv(tweets, to = &amp;quot;ASCII&amp;quot;, sub = &amp;quot; &amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the following code section, I will apply bunch of codes to remove special characters, hyperlink, usernames, tabs, punctuations and unnecessary white spaces. Because all these are not don’t have any relation to the topic modeling. I have mentioned specific use of each code along with the codes below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tweets &amp;lt;- gsub(&amp;quot;(RT|via)((?:\\b\\W*@\\w+)+)&amp;quot;, &amp;quot;&amp;quot;, tweets)  # Remove the &amp;quot;RT&amp;quot; (retweet) and usernames 
tweets = gsub(&amp;quot;http.+ |http.+$&amp;quot;, &amp;quot; &amp;quot;, tweets)  # Remove html links
tweets = gsub(&amp;quot;http[[:alnum:]]*&amp;quot;, &amp;quot;&amp;quot;, tweets)
tweets = gsub(&amp;quot;[[:punct:]]&amp;quot;, &amp;quot; &amp;quot;, tweets)  # Remove punctuation
tweets = gsub(&amp;quot;[ |\t]{2,}&amp;quot;, &amp;quot; &amp;quot;, tweets)  # Remove tabs
tweets = gsub(&amp;quot;^ &amp;quot;, &amp;quot;&amp;quot;, tweets)  # Leading blanks
tweets = gsub(&amp;quot; $&amp;quot;, &amp;quot;&amp;quot;, tweets)  # Lagging blanks
tweets = gsub(&amp;quot; +&amp;quot;, &amp;quot; &amp;quot;, tweets) # General spaces &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above bunch of cleaning codes we have removed html, username and so on. We saw our sample tweet had a html link in it. Let’s check if the transoformation worked properly or not:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(as.character(tweets[[1500]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Half a million Rohingya refugee children at risk in overcrowded camps in Bangladesh with cyclone and&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the punchtuations (.) and website link have been removed from our sample tweet as intended.&lt;/p&gt;
&lt;p&gt;I will convert all the tweets in lower case since in R words are case sensitive. For example: ‘Tweets’ and ‘tweets’ are considered as two different words. Moreover, I will remove the duplecate tweets. Among the tweets downloaded using twitter public API there duplicate tweets also exist. To make sure the tweets that are used here are not duplicated now I will remove the duplicated tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tweets = tolower(tweets)
tweets = unique(tweets)
writeLines(as.character(tweets[[1500]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## best quality underground metal detector in bangladesh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check I have extracted the 1500th tweet. But this time I have got a different tweet. Because after removing duplecate tweets I had left with 5,561 tweets out of 20,000 tweets which I started with. So the serial number of tweets have also changed.&lt;/p&gt;
&lt;p&gt;As the next step of data processing I will convert this tweets file, which is a character vector, into a corpus. In general term, corpus in linguistic means a structured set of texts that can be used for statistical analysis, hypothesis testing, occurance checking and validating linguistic rules. To To achive this goal I will use ‘corpus’ and ‘VectorSource’ commands from ‘tm’ library in R. While ‘VectorSource’ will interpret each element of our character vector file ‘tweets’ as a document and feed that input into ‘corpus’ command. Which eventually will convert that into corpus suitable for statistical analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tm)
corpus &amp;lt;- Corpus(VectorSource(tweets))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will do some more cleaning on the corpus by removing stop words and numbers because both these have very little value, if there is any, towards our goal of sentiment analylsis and topic modeling. For clarity I will explain a bit more on stop words here before going into coding. Stop words are some extremely common words used in a language which may carry very little value for a particular analysis. In this case I will use the stop words list comes along with ‘tm’ package. To get an idea of the list here are some example of stop words: a, about, above and so on. The exhaustive list can be found in this Github link: &lt;a href=&#34;https://github.com/arc12/Text-Mining-Weak-Signals/wiki/Standard-set-of-english-stopwords&#34; class=&#34;uri&#34;&gt;https://github.com/arc12/Text-Mining-Weak-Signals/wiki/Standard-set-of-english-stopwords&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpus &amp;lt;- tm_map(corpus, removeWords, stopwords(&amp;quot;english&amp;quot;))  
corpus &amp;lt;- tm_map(corpus, removeNumbers)
writeLines(as.character(corpus[[1500]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## best quality underground metal detector  bangladesh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see from our sample tweet that a bunch of stop words (in) is removed.&lt;/p&gt;
&lt;p&gt;At this step I will convert the words in the corpus into stem words. In general terms, word stemming means the process of reducing a word to its base form which may or may not be the same as the morphological root of the word or may or may not bear meaning by the stem word itself. For example, all these words: ‘fishing’, ‘fisheries’ can be reduced to ‘fish’ by a stemming algorithm. Here ‘fish’ bears a meaning. But on the other hand this bunch of words: ‘argue’, ‘argued’ can be reduced to ‘argu’ in this case the stem doesn’t bear any particular meaning. Stemming a document makes it easier to cluster words and make analysis since. In addition to the stemming I will also delete my search key ‘Bangladesh’ from the tweets. Since I am analyzing tweets containing Bangaldesh and ‘amp’, it’s illogical to keep the term ‘bangladesh’ since that’s the search term and ‘amp’is abbrebiation of ’Accelerated Mobile Page’ which is a part of html link that improved web surfing experience from mobile devices.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpus &amp;lt;- tm_map(corpus, stemDocument)
corpus = tm_map(corpus, removeWords, c(&amp;quot;bangladesh&amp;quot;,&amp;quot;amp&amp;quot;, &amp;quot;will&amp;quot;, &amp;#39;get&amp;#39;, &amp;#39;can&amp;#39;))

writeLines(as.character(corpus[[1500]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## best qualiti underground metal detector&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again a recheck of our sample tweet we can see ‘quality’ has been tranformed into their stem form: ‘qualiti’ and ‘bangladesh’ has been removed.&lt;/p&gt;
&lt;p&gt;I am finally done with our first step of data cleaning and pre-processing. On the next step I will start data processing to create our topic model. But before diving into model creation I decided to crate a word cloud to get a feel about the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wordcloud)
set.seed(1234)
palet  = brewer.pal(8, &amp;#39;Dark2&amp;#39;)
wordcloud(corpus, min.freq = 50, scale = c(4, 0.2) , random.order = TRUE, col = palet)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-23-twitter-sentiment-analysis-on-bangladesh_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the resulting word cloud we can see that the words are colored differently, which is based on the frequencies of the words appearing in the tweets. Looking at the most largest two fonts (black and green) we can find these words: rohingya, refuge, today, india, cricket, live. To interpret anything from such word cluster subject knowledge comes handy. Sinice I am from Bangladesh, I know that influx of Rohingya refugee from Myanmar is one of the most recent most discussed issue. Intuitively enough, Rohingya, refuge can be classified as related to the Rohingya crisis. On the other hand Cricket is the most popular game in Bangladesh along with other countries from south asian region. Cricket and Live can be thought to be related to Cricket. India and Today don’t have a general strong association with either of the two primary topics that we have sorted out. We will see how it goes in our further analysis in topic modeling.&lt;/p&gt;
&lt;p&gt;As a next processing step now I will convert our corpus in a Document Term Matrix (DTM). DTM creates a matrix that consists all words or terms as an individual column and each document, in our case each tweet, as a row. Numeric value of 1 is assigned to the words that apprear in the document from the corresponding row and value of 0 is assigned to the rest of the words in that row. Thus the resulting DTM file is a sparse which is a large matrix containing a lot of 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtm = DocumentTermMatrix(corpus)
dtm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;&amp;lt;DocumentTermMatrix (documents: 5561, terms: 8561)&amp;gt;&amp;gt;
## Non-/sparse entries: 44969/47562752
## Sparsity           : 100%
## Maximal term length: 30
## Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the file summary of ‘dtm’ file we can see that it contains total 5,561 document, which is the total number of tweets that we have, and total 8,565 term, which shows we have total 8,565 unique words in our tweets. From the non/sparse entries ratio and the percentage of Sparsity we can see that the sparsity of the file, which is not exactly 100 but very close to 100, is very very high which is means lot of words appeard only in few tweets. Let’s inspect the ‘dtm’ file to have a feel about the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doc.length = apply(dtm, 1, sum)
dtm = dtm[doc.length &amp;gt; 0,]
dtm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;&amp;lt;DocumentTermMatrix (documents: 5552, terms: 8561)&amp;gt;&amp;gt;
## Non-/sparse entries: 44969/47485703
## Sparsity           : 100%
## Maximal term length: 30
## Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspect(dtm[1:2,10:15])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;&amp;lt;DocumentTermMatrix (documents: 2, terms: 6)&amp;gt;&amp;gt;
## Non-/sparse entries: 6/6
## Sparsity           : 50%
## Maximal term length: 6
## Weighting          : term frequency (tf)
## Sample             :
##     Terms
## Docs dinesh india injur jan within year
##    1      0     0     0   0      1    1
##    2      1     1     1   1      0    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that out of the five terms first four terms were present in doc 2 and rest 2 terms were present in the doc 1. And accordingly the value of 1 and 0 have been distributed in the cells. Now let’s look at some of the most frequent words in our DTM.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
freq = colSums(as.matrix(dtm))
length(freq)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8561&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ord = order(freq, decreasing = TRUE)
freq[head(ord, n = 20)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rohingya     news  zimbabw    india   refuge     live  cricket    today 
##      590      509      465      324      308      299      297      296 
##      tri      new     camp     seri pakistan  myanmar    match   nation 
##      266      248      245      239      238      236      221      203 
##   bangla   wicket      odi    peopl 
##      201      187      182      182&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the list of 20 most frequent words we can see that Rohingya crisis and Cricket related terms are the most frequntly used terms. Which shows resemblance with what we saw in our wordcloud. We can now see how different words are associated. Since we see that Cricket and Rohingy are two frequntly used topics, we can try to see which words associate with these two words. For this we will use ‘findAssocs’ command from ‘tm’ package. To run this command we need to provde the benchmark term and then a minimum value of correlation, which can range from 0 to 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;findAssocs(dtm, &amp;quot;cricket&amp;quot;,0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $cricket
##     cup zimbabw   score    team 
##    0.23    0.22    0.20    0.20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;findAssocs(dtm, &amp;#39;rohingya&amp;#39;, 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $rohingya
##   refuge     camp  myanmar  repatri    crisi     hous children 
##     0.51     0.46     0.41     0.33     0.25     0.25     0.24&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;findAssocs(dtm, &amp;#39;news&amp;#39;, 0.2 )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $news
##   today  latest  bangla   updat januari     atn  decemb ekattor  jamuna ekushey 
##    0.77    0.77    0.76    0.68    0.56    0.53    0.34    0.33    0.28    0.26 
##    post channel 
##    0.21    0.21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From our resulting associations for both the words, we can see Cricket is associated with the words cup, zimbabwe, score and team. Which makes proper sense because every other words except Zimbabwe are related to game and Zimbabwe is one of the common team with whom Bangladesh have had quite a lot of cricket matches (such insights come from subject matter knowledge!). On the other hand, with the word Rohingya we can see assiciated words camp, refugee, myanmar, repatriation etc. evolve around the crisis created by the Rohingya refugees coming from Bangladesh’s neighboring country Myanmar.&lt;/p&gt;
&lt;p&gt;I will plot the most frequest 100 words now in a barplot to visually see how their frequencies are distributed. Checking the list of frequent words in list and graphically has two benefits: firstly, it gives a feeling about the analysis and secondly, it puts some sort of control on the quality of data cleaning done in previous steps. For example, after generating the most frequent words I found some of the words such as: amp, will, can, get are not removed. So I went back and added these words in the word remove step of data cleaning.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot = data.frame(words = names(freq), count = freq)
library(ggplot2)
plot = subset(plot, plot$count &amp;gt; 150) #creating a subset of words having more than 100 frequency
str(plot)
ggplot(data = plot, aes(words, count)) + geom_bar(stat = &amp;#39;identity&amp;#39;) + ggtitle(&amp;#39;Words used more than 150 times&amp;#39;)+coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-23-twitter-sentiment-analysis-on-bangladesh_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;topic-modeling-using-lda&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Topic modeling using LDA&lt;/h1&gt;
&lt;p&gt;I have used ‘topicmodel’ package available in R for topic modeling.
As discussed earlier, in LDA model number of topics are to be selected. Based on which LDA model creates the probability of each topic in each document and also distributes the words under each topic. Selecting more number of topics may result in more grannular segregation but at the same time the differences among different topics may get blurred. While on the other hand selecting very small number of topic can lead to losing possible topic. So to minimze this error I tried three different K or number of topics to create my LDA model. I used 2, 5, 10 as the number of topics and created three different LDA models based on these K values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(topicmodels)
#LDA model with 5 topics selected
lda_5 = LDA(dtm, k = 5, method = &amp;#39;Gibbs&amp;#39;, 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 2 topics selected
lda_2 = LDA(dtm, k = 2, method = &amp;#39;Gibbs&amp;#39;, 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
lda_10 = LDA(dtm, k = 10, method = &amp;#39;Gibbs&amp;#39;, 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;LDA model produces a good bulk of information. But getting the most frequent words under each topic and document wise probability of each topic are the two most important pieces of information that I can use for my analysis purpose. First of all I will fetch top 10 terms in each topic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Top 10 terms or words under each topic
top10terms_5 = as.matrix(terms(lda_5,10))
top10terms_2 = as.matrix(terms(lda_2,10))
top10terms_10 = as.matrix(terms(lda_10,10))

top10terms_5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Topic 1   Topic 2    Topic 3   Topic 4    Topic 5 
##  [1,] &amp;quot;news&amp;quot;    &amp;quot;rohingya&amp;quot; &amp;quot;zimbabw&amp;quot; &amp;quot;india&amp;quot;    &amp;quot;new&amp;quot;   
##  [2,] &amp;quot;live&amp;quot;    &amp;quot;refuge&amp;quot;   &amp;quot;cricket&amp;quot; &amp;quot;pakistan&amp;quot; &amp;quot;one&amp;quot;   
##  [3,] &amp;quot;today&amp;quot;   &amp;quot;camp&amp;quot;     &amp;quot;tri&amp;quot;     &amp;quot;peopl&amp;quot;    &amp;quot;year&amp;quot;  
##  [4,] &amp;quot;bangla&amp;quot;  &amp;quot;myanmar&amp;quot;  &amp;quot;seri&amp;quot;    &amp;quot;countri&amp;quot;  &amp;quot;day&amp;quot;   
##  [5,] &amp;quot;dhaka&amp;quot;   &amp;quot;girl&amp;quot;     &amp;quot;match&amp;quot;   &amp;quot;like&amp;quot;     &amp;quot;see&amp;quot;   
##  [6,] &amp;quot;januari&amp;quot; &amp;quot;children&amp;quot; &amp;quot;nation&amp;quot;  &amp;quot;time&amp;quot;     &amp;quot;just&amp;quot;  
##  [7,] &amp;quot;now&amp;quot;     &amp;quot;muslim&amp;quot;   &amp;quot;wicket&amp;quot;  &amp;quot;indian&amp;quot;   &amp;quot;work&amp;quot;  
##  [8,] &amp;quot;latest&amp;quot;  &amp;quot;say&amp;quot;      &amp;quot;odi&amp;quot;     &amp;quot;take&amp;quot;     &amp;quot;week&amp;quot;  
##  [9,] &amp;quot;updat&amp;quot;   &amp;quot;sex&amp;quot;      &amp;quot;banvzim&amp;quot; &amp;quot;don&amp;quot;      &amp;quot;follow&amp;quot;
## [10,] &amp;quot;love&amp;quot;    &amp;quot;million&amp;quot;  &amp;quot;world&amp;quot;   &amp;quot;nepal&amp;quot;    &amp;quot;last&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top10terms_2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Topic 1    Topic 2   
##  [1,] &amp;quot;rohingya&amp;quot; &amp;quot;news&amp;quot;    
##  [2,] &amp;quot;refuge&amp;quot;   &amp;quot;zimbabw&amp;quot; 
##  [3,] &amp;quot;camp&amp;quot;     &amp;quot;india&amp;quot;   
##  [4,] &amp;quot;myanmar&amp;quot;  &amp;quot;live&amp;quot;    
##  [5,] &amp;quot;peopl&amp;quot;    &amp;quot;cricket&amp;quot; 
##  [6,] &amp;quot;girl&amp;quot;     &amp;quot;today&amp;quot;   
##  [7,] &amp;quot;countri&amp;quot;  &amp;quot;tri&amp;quot;     
##  [8,] &amp;quot;children&amp;quot; &amp;quot;new&amp;quot;     
##  [9,] &amp;quot;like&amp;quot;     &amp;quot;seri&amp;quot;    
## [10,] &amp;quot;one&amp;quot;      &amp;quot;pakistan&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top10terms_10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Topic 1       Topic 2           Topic 3    Topic 4   Topic 5   Topic 6
##  [1,] &amp;quot;time&amp;quot;        &amp;quot;now&amp;quot;             &amp;quot;india&amp;quot;    &amp;quot;cricket&amp;quot; &amp;quot;zimbabw&amp;quot; &amp;quot;girl&amp;quot; 
##  [2,] &amp;quot;dhaka&amp;quot;       &amp;quot;one&amp;quot;             &amp;quot;pakistan&amp;quot; &amp;quot;world&amp;quot;   &amp;quot;tri&amp;quot;     &amp;quot;sex&amp;quot;  
##  [3,] &amp;quot;bangladeshi&amp;quot; &amp;quot;report&amp;quot;          &amp;quot;like&amp;quot;     &amp;quot;team&amp;quot;    &amp;quot;seri&amp;quot;    &amp;quot;love&amp;quot; 
##  [4,] &amp;quot;make&amp;quot;        &amp;quot;work&amp;quot;            &amp;quot;nepal&amp;quot;    &amp;quot;run&amp;quot;     &amp;quot;match&amp;quot;   &amp;quot;women&amp;quot;
##  [5,] &amp;quot;two&amp;quot;         &amp;quot;watch&amp;quot;           &amp;quot;hindus&amp;quot;   &amp;quot;canada&amp;quot;  &amp;quot;nation&amp;quot;  &amp;quot;nude&amp;quot; 
##  [6,] &amp;quot;islam&amp;quot;       &amp;quot;just&amp;quot;            &amp;quot;south&amp;quot;    &amp;quot;start&amp;quot;   &amp;quot;wicket&amp;quot;  &amp;quot;video&amp;quot;
##  [7,] &amp;quot;high&amp;quot;        &amp;quot;right&amp;quot;           &amp;quot;want&amp;quot;     &amp;quot;day&amp;quot;     &amp;quot;odi&amp;quot;     &amp;quot;kill&amp;quot; 
##  [8,] &amp;quot;state&amp;quot;       &amp;quot;indian&amp;quot;          &amp;quot;think&amp;quot;    &amp;quot;cup&amp;quot;     &amp;quot;first&amp;quot;   &amp;quot;fuck&amp;quot; 
##  [9,] &amp;quot;also&amp;quot;        &amp;quot;mishalhusainbbc&amp;quot; &amp;quot;back&amp;quot;     &amp;quot;score&amp;quot;   &amp;quot;banvzim&amp;quot; &amp;quot;porn&amp;quot; 
## [10,] &amp;quot;much&amp;quot;        &amp;quot;visit&amp;quot;           &amp;quot;take&amp;quot;     &amp;quot;play&amp;quot;    &amp;quot;win&amp;quot;     &amp;quot;dhaka&amp;quot;
##       Topic 7   Topic 8    Topic 9 Topic 10   
##  [1,] &amp;quot;news&amp;quot;    &amp;quot;rohingya&amp;quot; &amp;quot;peopl&amp;quot; &amp;quot;new&amp;quot;      
##  [2,] &amp;quot;today&amp;quot;   &amp;quot;refuge&amp;quot;   &amp;quot;year&amp;quot;  &amp;quot;countri&amp;quot;  
##  [3,] &amp;quot;live&amp;quot;    &amp;quot;camp&amp;quot;     &amp;quot;help&amp;quot;  &amp;quot;see&amp;quot;      
##  [4,] &amp;quot;bangla&amp;quot;  &amp;quot;myanmar&amp;quot;  &amp;quot;need&amp;quot;  &amp;quot;week&amp;quot;     
##  [5,] &amp;quot;januari&amp;quot; &amp;quot;children&amp;quot; &amp;quot;home&amp;quot;  &amp;quot;follow&amp;quot;   
##  [6,] &amp;quot;latest&amp;quot;  &amp;quot;muslim&amp;quot;   &amp;quot;give&amp;quot;  &amp;quot;last&amp;quot;     
##  [7,] &amp;quot;updat&amp;quot;   &amp;quot;say&amp;quot;      &amp;quot;look&amp;quot;  &amp;quot;england&amp;quot;  
##  [8,] &amp;quot;sri&amp;quot;     &amp;quot;million&amp;quot;  &amp;quot;babi&amp;quot;  &amp;quot;australia&amp;quot;
##  [9,] &amp;quot;lanka&amp;quot;   &amp;quot;support&amp;quot;  &amp;quot;sinc&amp;quot;  &amp;quot;bts&amp;quot;      
## [10,] &amp;quot;post&amp;quot;    &amp;quot;repatri&amp;quot;  &amp;quot;even&amp;quot;  &amp;quot;set&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that all three models picked the topics of Cricket and Rohingiya. But models with 5 and 10 topics also picked some other topics anlong with these two. From which we can see the application of the previous discussion about grannularity vs generalization. If we look at the top words from all the topics created from model with 10 topics, we can see that overall there is a lack of coherence among the words inside each topic. Similar observation can be made for the model with 5 topics. While the model with 2 topics provide two topics with a compact coherence among the topics. Another important thing to notice is that how the model with 10 topic picked some topic that were ignored by the model with 2 and 5 topics. Such as nudity (topic-6)!&lt;/p&gt;
&lt;p&gt;Since we can clearly see that the topics of ‘Rohingya Crisis’ and ‘Cricket’ are two most common topics, I will move with these topic for further analysis.&lt;/p&gt;
&lt;p&gt;Topics found out by our model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lda.topics_5 = as.matrix(topics(lda_5))
lda.topics_2 = as.matrix(topics(lda_2))
lda.topics_10 = as.matrix(topics(lda_10))
#write.csv(lda.topics_5,file = paste(&amp;#39;LDAGibbs&amp;#39;,5,&amp;#39;DocsToTopics.csv&amp;#39;))
#write.csv(lda.topics_2,file = paste(&amp;#39;LDAGibbs&amp;#39;,2,&amp;#39;DocsToTopics.csv&amp;#39;))
#write.csv(lda.topics_10,file = paste(&amp;#39;LDAGibbs&amp;#39;,10,&amp;#39;DocsToTopics.csv&amp;#39;))

summary(as.factor(lda.topics_5[,1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    1    2    3    4    5 
## 1208 1293 1230 1003  818&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(as.factor(lda.topics_2[,1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    1    2 
## 3151 2401&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(as.factor(lda.topics_10[,1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   1   2   3   4   5   6   7   8   9  10 
## 755 659 607 577 708 546 385 593 364 358&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also get document wise probability of each topic. I have created three files for each of my model and also saved the output for future use. Probability of each topic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;topicprob_5 = as.matrix(lda_5@gamma)
topicprob_2 = as.matrix(lda_2@gamma)
topicprob_10 = as.matrix(lda_10@gamma)

#write.csv(topicprob_5, file = paste(&amp;#39;LDAGibbs&amp;#39;, 5, &amp;#39;DoctToTopicProb.csv&amp;#39;))
#write.csv(topicprob_2, file = paste(&amp;#39;LDAGibbs&amp;#39;, 2, &amp;#39;DoctToTopicProb.csv&amp;#39;))
#write.csv(topicprob_10, file = paste(&amp;#39;LDAGibbs&amp;#39;, 10, &amp;#39;DoctToTopicProb.csv&amp;#39;))

head(topicprob_2,1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 0.5409836 0.4590164&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a sample we can see that according to my model with 5 topics, how document 1 has different probabilities of containing each topic. The highest probability from topic-2. Accordingly the model classified as representing topic-2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)
library(dplyr)
library(ggplot2)

#Tokenizing character vector file &amp;#39;tweets&amp;#39;.
token = data.frame(text=tweets, stringsAsFactors = FALSE) %&amp;gt;% unnest_tokens(word, text)

#Matching sentiment words from the &amp;#39;NRC&amp;#39; sentiment lexicon
senti = inner_join(token, get_sentiments(&amp;quot;nrc&amp;quot;)) %&amp;gt;%
  count(sentiment)
senti$percent = (senti$n/sum(senti$n))*100

#Plotting the sentiment summary 
ggplot(senti, aes(sentiment, percent)) +   
        geom_bar(aes(fill = sentiment), position = &amp;#39;dodge&amp;#39;, stat = &amp;#39;identity&amp;#39;)+ 
        ggtitle(&amp;quot;Sentiment analysis based on lexicon: &amp;#39;NRC&amp;#39;&amp;quot;)+
  coord_flip() +
        theme(legend.position = &amp;#39;none&amp;#39;, plot.title = element_text(size=18, face = &amp;#39;bold&amp;#39;),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face=&amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-23-twitter-sentiment-analysis-on-bangladesh_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;additional-analysis-sentiment-analysis-on-rohingya-topic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Additional analysis: Sentiment analysis on Rohingya topic&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tm)
library(quanteda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Package version: 2.0.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parallel computing: 2 of 4 threads used.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See https://quanteda.io for tutorials and examples.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;quanteda&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:tm&amp;#39;:
## 
##     as.DocumentTermMatrix, stopwords&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:NLP&amp;#39;:
## 
##     meta, meta&amp;lt;-&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:utils&amp;#39;:
## 
##     View&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpus_roh = corpus(tweets)
corpus_roh = (corpus_roh = subset(corpus_roh, grepl(&amp;#39;rohingya&amp;#39;, texts(corpus_roh))))
writeLines(as.character(corpus_roh[[150]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## the suffering in the rohingya refugee camp in bangladesh is indescribable&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Tokenizing character vector file &amp;#39;tweets&amp;#39;.
library(tidytext)
token_roh = tibble(text=corpus_roh)  %&amp;gt;% unnest_tokens(word, text, format = &amp;quot;text&amp;quot;)
 
#Matching sentiment words from the &amp;#39;NRC&amp;#39; sentiment lexicon
library(dplyr)
senti_roh = inner_join(token_roh, get_sentiments(&amp;quot;nrc&amp;quot;)) %&amp;gt;%
  count(sentiment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;word&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;senti_roh$percent = (senti_roh$n/sum(senti_roh$n))*100

#Plotting the sentiment summary 
library(ggplot2)
ggplot(senti_roh, aes(sentiment, percent)) +   
        geom_bar(aes(fill = sentiment), position = &amp;#39;dodge&amp;#39;, stat = &amp;#39;identity&amp;#39;)+ 
        ggtitle(&amp;quot;Sentiment analysis based on lexicon: &amp;#39;NRC&amp;#39;&amp;quot;)+
  coord_flip() +
        theme(legend.position = &amp;#39;none&amp;#39;, plot.title = element_text(size=18, face = &amp;#39;bold&amp;#39;),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face=&amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-23-twitter-sentiment-analysis-on-bangladesh_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;overall-finding-and-discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overall finding and discussion&lt;/h1&gt;
&lt;p&gt;From our random walk on the tweets related to Bangladesh, we have seen ‘cricket’ and ‘Rohingya’ were the two areas that people cared most. Overall, people exuded a positive sentiment along with emotions of trust and anticipation. But in case of Rohingya crisis, we showed a mixed sentiment. About Rohingya issue both the positive and negative sentiments were high. Moreover, the emotions also seemed to be mixed. People felt sorry for the Rohingya people but they also expressed fear. So, what could that mean? Were people sympathetic to the plight of Rohingya but also had some share of fear related to the issue? This study doesn’t allow us to conclude on any such conclusion. But it gives us an idea of what we may achieve by having such a walk. Maybe we need to go for a walk with Bangladesh on the online social media sites more often to get a clearer image on what people talk about Bangladesh and what may needs to be improved to leave a better digital footprint for the country.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
